{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: 05_simple_training.ipynb -> simple_training.py\r\n",
      "1 notebook(s) exported into folder: .\r\n"
     ]
    }
   ],
   "source": [
    "!python -m jupytools export -nb \"05_simple_training.ipynb\" -o ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "from os.path import dirname, join\n",
    "from functools import reduce\n",
    "from pdb import set_trace\n",
    "\n",
    "import cv2 as cv\n",
    "import jupytools\n",
    "import jupytools.syspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pretrainedmodels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "jupytools.syspath.add(join(dirname(os.getcwd()), 'protein_project'))\n",
    "jupytools.syspath.add('rxrx1-utils')\n",
    "if jupytools.is_notebook():\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "else:\n",
    "    from tqdm import tqdm as tdqm\n",
    "    \n",
    "import contextlib, io\n",
    "with contextlib.redirect_stderr(io.StringIO()):\n",
    "    # from augmentation import Augmented, multichannel_norm\n",
    "    from basedir import ROOT, NUM_CLASSES\n",
    "    from catalyst_visdom import BatchMetricsPlotCallback, EpochMetricsPlotCallback\n",
    "    from dataset import load_data, build_stats_index, RxRxDataset\n",
    "    from model.utils import freeze_all, unfreeze_layers\n",
    "    from visual import rgb, six, show_1, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "data_dict = load_data(filenames=['train.json', 'test.json'])\n",
    "train_records, test_records = data_dict['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bernoulli(p): return int(np.random.binomial(1, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseTransform:\n",
    "    def __init__(self, apply_always=False, p=1.0, key='features'):\n",
    "        self.apply_always = apply_always\n",
    "        self.p = p\n",
    "        self.key = key\n",
    "    def __call__(self, x):\n",
    "        if self.apply_always or bernoulli(self.p):\n",
    "            return self.apply(x)\n",
    "        return x\n",
    "    def apply(self, x):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnconditionalTransform(BaseTransform):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(apply_always=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Compose(UnconditionalTransform):\n",
    "    def __init__(self, transforms):\n",
    "        super().__init__(apply_always=True)\n",
    "        self.transforms = transforms\n",
    "    def apply(self, dataset_record):\n",
    "        return reduce(lambda x, f: f(x), self.transforms, dataset_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class JoinChannels(UnconditionalTransform):\n",
    "    def apply(self, record):\n",
    "        record['features'] = np.stack(list(record['features'].values()))\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Resize(UnconditionalTransform):\n",
    "    def __init__(self, sz):\n",
    "        super().__init__()\n",
    "        self.sz = (sz, sz) if isinstance(sz, int) else sz\n",
    "    def apply(self, record):\n",
    "        record['features'] = cv.resize(record['features'], self.sz) \n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToFloat(UnconditionalTransform):\n",
    "    def apply(self, record):\n",
    "        record['features'] = record['features'].astype(np.float32)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SwapChannels(UnconditionalTransform):\n",
    "    def __init__(self, new_order):\n",
    "        super().__init__()\n",
    "        self.new_order = new_order\n",
    "    def apply(self, record):\n",
    "        record['features'] = record['features'].transpose(*self.new_order)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VerticalFlip(BaseTransform):\n",
    "    def apply(self, record): \n",
    "        record['features'] = cv.flip(record['features'], 0)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HorizontalFlip(BaseTransform):\n",
    "    def apply(self, record): \n",
    "        record['features'] = cv.flip(record['features'], 1)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PixelStatsNorm(UnconditionalTransform):\n",
    "    def __init__(self, stats):\n",
    "        super().__init__()\n",
    "        self.stats = stats\n",
    "    def apply(self, record):\n",
    "        keys = [(record['id_code'], i+1, record['site']) for i in range(6)]\n",
    "        mean = np.stack([self.stats[key]['mean'] for key in keys]).astype(np.float32)\n",
    "        std = np.stack([self.stats[key]['std'] for key in keys]).astype(np.float32)\n",
    "        mean, std = [arr.reshape(len(arr), 1, 1) for arr in (mean, std)]\n",
    "        norm = (record['features'] - mean)/(std + 1e-8)\n",
    "        record['features'] = norm\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Augmented(Dataset):\n",
    "    def __init__(self, ds, tr):\n",
    "        self.ds = ds\n",
    "        self.tr = tr\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, index):\n",
    "        return self.tr(self.ds[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition and Normalization Stats Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "stats = build_stats_index(ROOT/'pixel_stats.csv')\n",
    "\n",
    "train_transform = Compose([\n",
    "    JoinChannels(),\n",
    "    SwapChannels((1, 2, 0)),\n",
    "    Resize(224),\n",
    "    ToFloat(),\n",
    "    VerticalFlip(p=0.25),\n",
    "    HorizontalFlip(p=0.25),\n",
    "    SwapChannels((2, 0, 1)),\n",
    "    PixelStatsNorm(stats)\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    JoinChannels(),\n",
    "    SwapChannels((1, 2, 0)),\n",
    "    Resize(224),\n",
    "    ToFloat(),\n",
    "    SwapChannels((2, 0, 1)),\n",
    "    PixelStatsNorm(stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HEPG2-01_1_B02', 1, 1),\n",
       " ('HEPG2-01_1_B02', 2, 1),\n",
       " ('HEPG2-01_1_B02', 3, 1),\n",
       " ('HEPG2-01_1_B02', 4, 1),\n",
       " ('HEPG2-01_1_B02', 5, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stats)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split(records):\n",
    "    experiments = np.unique([r['experiment'] for r in records])\n",
    "    cell_types = np.unique([r['cell_type'] for r in records])\n",
    "    holdouts = []\n",
    "    for ct in cell_types:\n",
    "        holdout = np.random.choice(experiments[\n",
    "            [exp.startswith(ct) for exp in experiments]])\n",
    "        holdouts.append(holdout)\n",
    "\n",
    "    train_exp, valid_exp = [exp for exp in experiments if exp not in holdouts], holdouts\n",
    "    print(f'Train experiments: {train_exp}')\n",
    "    print(f'Valid experiments: {valid_exp}')\n",
    "    train = [r for r in records if r['experiment'] in train_exp]\n",
    "    valid = [r for r in records if r['experiment'] in valid_exp]\n",
    "    print(f'Train size: {len(train)}, valid size: {len(valid)}')\n",
    "    \n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train experiments: ['HEPG2-02', 'HEPG2-03', 'HEPG2-04', 'HEPG2-05', 'HEPG2-06', 'HEPG2-07', 'HUVEC-01', 'HUVEC-02', 'HUVEC-03', 'HUVEC-04', 'HUVEC-06', 'HUVEC-07', 'HUVEC-08', 'HUVEC-09', 'HUVEC-10', 'HUVEC-11', 'HUVEC-12', 'HUVEC-13', 'HUVEC-14', 'HUVEC-15', 'HUVEC-16', 'RPE-01', 'RPE-02', 'RPE-04', 'RPE-05', 'RPE-06', 'RPE-07', 'U2OS-02', 'U2OS-03']\n",
      "Valid experiments: ['HEPG2-01', 'HUVEC-05', 'RPE-03', 'U2OS-01']\n",
      "Train size: 64170, valid size: 8860\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "bs = 650\n",
    "n_cpu = 12\n",
    "\n",
    "train_records, valid_records = split(train_records)\n",
    "\n",
    "trn_ds = RxRxDataset(train_records, num_classes=NUM_CLASSES)\n",
    "aug_trn_ds = Augmented(trn_ds, train_transform)\n",
    "\n",
    "val_ds = RxRxDataset(valid_records, num_classes=NUM_CLASSES)\n",
    "aug_val_ds = Augmented(val_ds, train_transform)\n",
    "\n",
    "tst_ds = RxRxDataset(test_records, num_classes=NUM_CLASSES)\n",
    "aug_tst_ds = Augmented(tst_ds, test_transform)\n",
    "\n",
    "loaders = OrderedDict([\n",
    "    ('train', DataLoader(aug_trn_ds, batch_size=bs, shuffle=True, num_workers=n_cpu)),\n",
    "    ('valid', DataLoader(aug_val_ds, batch_size=bs, shuffle=False, num_workers=n_cpu)),\n",
    "])\n",
    "\n",
    "test_loader = DataLoader(aug_tst_ds, batch_size=bs, shuffle=False, num_workers=n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from catalyst.contrib.modules import GlobalConcatPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_model(model_name):\n",
    "    return pretrainedmodels.__dict__[model_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DoubleModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.m1 = create_model(model_name)\n",
    "        self.m2 = create_model(model_name)\n",
    "        self.pool = GlobalConcatPool2d()\n",
    "        \n",
    "        out_features = self.m1.last_linear.in_features * 2\n",
    "        \n",
    "        self.top = nn.Sequential(\n",
    "            nn.Linear(out_features * 2, out_features),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(out_features, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t1 = self.pool(self.m1.features(x[:, 0:3, :]))\n",
    "        t2 = self.pool(self.m2.features(x[:, 3:6, :]))\n",
    "        feat = torch.cat([t1, t2], dim=1)\n",
    "        logits = self.top(feat.squeeze())\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SixChannelModel(nn.Module):\n",
    "    \n",
    "#     def __init__(self, model_name, num_classes):\n",
    "#         super().__init__()\n",
    "#         model = create_model(model_name)\n",
    "#         feat_dim = model.last_linear.in_features\n",
    "#         model.last_linear = nn.Sequential(\n",
    "#             nn.Linear(feat_dim, feat_dim),\n",
    "#             nn.LeakyReLU(negative_slope=0.01),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(feat_dim, num_classes)\n",
    "#         )\n",
    "#         conv0 = model.features.conv0\n",
    "#         new_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)\n",
    "#         new_conv.weight.data[:,0:3,:] = conv0.weight.data.clone()\n",
    "#         new_conv.weight.data[:,3:6,:] = conv0.weight.data.clone()\n",
    "#         model.features.conv0 = new_conv\n",
    "#         del conv0\n",
    "#         self.model = model\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SixChannelModel('densenet121', NUM_CLASSES)\n",
    "# freeze_all(model)\n",
    "# unfreeze_layers(model, ['model.last_linear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing layer top\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "model = DoubleModel('densenet121', NUM_CLASSES)\n",
    "freeze_all(model)\n",
    "unfreeze_layers(model, ['top'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RollingLoss:\n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "        self.prev = 0\n",
    "    def __call__(self, curr, batch_no):\n",
    "        a = self.smooth\n",
    "        avg_loss = a*self.prev + (1 - a)*curr\n",
    "        debias_loss = avg_loss/(1 - a**batch_no)\n",
    "        self.prev = avg_loss\n",
    "        return debias_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catalyst.contrib.schedulers import OneCycleLR\n",
    "# from catalyst.data.dataset import ListDataset\n",
    "# from catalyst.dl.callbacks import AccuracyCallback, AUCCallback, F1ScoreCallback\n",
    "# from catalyst.dl.callbacks import EarlyStoppingCallback\n",
    "# from catalyst.dl.runner import SupervisedRunner\n",
    "# from catalyst.utils import get_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = torch.optim.SGD(params=[\n",
    "#     dict(params=model.model.last_linear.parameters(), lr=0.001)\n",
    "# ])\n",
    "# sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "# runner = SupervisedRunner()\n",
    "# epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# runner.train(\n",
    "#     model=model,\n",
    "#     num_epochs=epochs,\n",
    "#     criterion=loss_fn,\n",
    "#     optimizer=opt,\n",
    "#     scheduler=sched,\n",
    "#     logdir='/home/ck/logs/protein/catalyst',\n",
    "#     loaders=loaders,\n",
    "#     callbacks=[\n",
    "#         AccuracyCallback(num_classes=NUM_CLASSES, accuracy_args=[1]),\n",
    "#         EarlyStoppingCallback(patience=10, minimize=False, metric='accuracy01'),\n",
    "#         BatchMetricsPlotCallback(use_env_creds=True),\n",
    "#         EpochMetricsPlotCallback(use_env_creds=True)\n",
    "#     ],\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0914 12:13:55.860801 140442564388672 __init__.py:505] Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]\n",
      "train metrics: accuracy=0.008%, loss=7.2169\n",
      "Score improved!\n",
      "valid metrics: accuracy=0.003%, loss=7.1448\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [2/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-28:\n",
      "Process Process-25:\n",
      "Process Process-35:\n",
      "Process Process-33:\n",
      "Process Process-34:\n",
      "Process Process-26:\n",
      "Process Process-36:\n",
      "Process Process-27:\n",
      "Process Process-31:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fbabd93cd90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-711a2055b1d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#export\n",
    "from visdom import Visdom\n",
    "\n",
    "epochs = 1000\n",
    "patience = 10\n",
    "\n",
    "opt = torch.optim.SGD(params=[\n",
    "    dict(params=model.top.parameters(), lr=0.001)\n",
    "])\n",
    "\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=10, mode='max')\n",
    "model = model.to(device)\n",
    "rolling_loss = dict(train=RollingLoss(), valid=RollingLoss())\n",
    "steps = dict(train=0, valid=0)\n",
    "\n",
    "trials = 0\n",
    "best_metric = -np.inf\n",
    "history = []\n",
    "stop = False\n",
    "\n",
    "vis = Visdom(server='0.0.0.0', port=9090,\n",
    "             username=os.environ['VISDOM_USERNAME'],\n",
    "             password=os.environ['VISDOM_PASSWORD'])\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'Epoch [{epoch}/{epochs}]')\n",
    "    iteration = dict(epoch=epoch, train_loss=list(), valid_loss=list())\n",
    "    \n",
    "    for name, loader in loaders.items():\n",
    "        is_training = name == 'train'\n",
    "        count = 0\n",
    "        metric = 0.0\n",
    "        \n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            for batch in loader:\n",
    "                steps[name] += 1\n",
    "                opt.zero_grad()\n",
    "                x = batch['features'].to(device)\n",
    "                y = batch['targets'].to(device)\n",
    "                out = model(x)\n",
    "                \n",
    "                if is_training:\n",
    "                    loss = loss_fn(out, y)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                \n",
    "                avg_loss = rolling_loss[name](loss.item(), steps[name])\n",
    "                iteration[f'{name}_loss'].append(avg_loss)\n",
    "                y_pred = out.softmax(dim=1).argmax(dim=1)\n",
    "                acc = (y_pred == y).float().mean().item()\n",
    "                metric += acc\n",
    "                count += len(batch)\n",
    "                vis.line(X=[steps[name]], Y=[avg_loss], name=f'{name}_loss', \n",
    "                         win=f'{name}_loss', update='append', \n",
    "                         opts=dict(title=f'Running Loss [{name}]'))\n",
    "        \n",
    "        metric /= count\n",
    "        sched.step(metric)\n",
    "        iteration[f'{name}_acc'] = metric\n",
    "        vis.line(X=[epoch], Y=[avg_loss], name=f'{name}', win='avg_loss',\n",
    "                 update='append', opts=dict(title='Average Epoch Loss'))\n",
    "        vis.line(X=[epoch], Y=[metric], name=f'{name}', win='accuracy', \n",
    "                 update='append', opts=dict(title=f'Accuracy'))\n",
    "        \n",
    "        if not is_training:\n",
    "            if metric > best_metric:\n",
    "                trials = 0\n",
    "                best_metric = metric\n",
    "                print('Score improved!')\n",
    "                torch.save(model.state_dict(), f'train.{epoch}.pth')\n",
    "            else:\n",
    "                trials += 1\n",
    "                if trials >= patience:\n",
    "                    stop = True\n",
    "                    break\n",
    "        \n",
    "        last_loss = iteration[f'{name}_loss'][-1]\n",
    "        \n",
    "        print(f'{name} metrics: accuracy={metric:2.3%}, loss={last_loss:.4f}')\n",
    "    \n",
    "    history.append(iteration)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    \n",
    "    if stop:\n",
    "        print(f'Early stopping on epoch: {epoch}')\n",
    "        break\n",
    "\n",
    "torch.save(history, 'history.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf *.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m jupytools export -nb \"03_cell_type_refactor.ipynb\" -o ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 17:27:18.908218 139690980988736 compression.py:14] lz4 not available, disabling compression. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from functools import partial\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from pdb import set_trace\n",
    "\n",
    "import cv2\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as T\n",
    "from catalyst.contrib.schedulers import OneCycleLR\n",
    "from catalyst.contrib.modules import GlobalConcatPool2d\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import AccuracyCallback\n",
    "from catalyst.dl.core import Callback\n",
    "from catalyst.utils import get_one_hot\n",
    "import pretrainedmodels\n",
    "from visdom import Visdom\n",
    "\n",
    "from data_bunch import rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def six(rec):\n",
    "    return np.stack([imread(filename) for _, filename in rec['images']])\n",
    "\n",
    "def rgb(img):\n",
    "    if img.shape[0] == 6:\n",
    "        img = img.transpose(1, 2, 0)\n",
    "    return rio.convert_tensor_to_rgb(img)\n",
    "\n",
    "def show_1(img, ax=None):\n",
    "    if img.shape[0] == 6:\n",
    "        img = rgb(img)\n",
    "    if img.shape[0] == 3:\n",
    "        img = img.transpose(1, 2, 0)\n",
    "    elif img.shape[-1] != 3:\n",
    "        raise ValueError(f'wrong image shape: {img.shape}')\n",
    "    if ax is not None:\n",
    "        ax.imshow(img)\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    \n",
    "def show(img, *imgs, titles=None, ncols=4):\n",
    "    from itertools import zip_longest\n",
    "    imgs = [img] + list(imgs)\n",
    "    titles = titles or []\n",
    "    nrows = int(np.ceil(len(imgs) / ncols))\n",
    "    sz = max(nrows, ncols)*2\n",
    "    f, axes = plt.subplots(nrows, ncols, figsize=(sz, sz))\n",
    "    for ax, img, t in zip_longest(axes.flat, imgs, titles):\n",
    "        if img is not None:\n",
    "            show_1(img, ax)\n",
    "        ax.set_title(t)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "    f.subplots_adjust(wspace=0, hspace=0.5)\n",
    "    return f\n",
    "\n",
    "def sample(records, count=1):\n",
    "    n = len(records)\n",
    "    idx = np.random.choice(n, count)\n",
    "    return [records[i] for i in idx]\n",
    "\n",
    "def visualize(records, n=25):\n",
    "    samples = sample(records, n)\n",
    "    titles = [\"{}\\n{}\".format(s['cell_type'], s['well_type']) for s in samples]\n",
    "    show(*map(six, samples), titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def list_files(folder):\n",
    "    dirname = os.path.expanduser(folder)\n",
    "    return [os.path.join(dirname, x) for x in os.listdir(dirname)]\n",
    "\n",
    "def load_data(filenames=None):\n",
    "\n",
    "    def treatment_only(records): \n",
    "        return [r for r in records if r['well_type'] == 'treatment']\n",
    "    \n",
    "    def re_encode(records):\n",
    "        enc = LabelEncoder()\n",
    "        labels = [r['sirna'] for r in records]\n",
    "        encoded = enc.fit_transform(labels)\n",
    "        for enc, r in zip(encoded, records):\n",
    "            r['enc_sirna'] = enc\n",
    "        return enc\n",
    "\n",
    "    if filenames is None:\n",
    "        filenames = [f'{fn}.json' for fn in ('train', 'test')]\n",
    "    \n",
    "    data, encoders = [], []\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            records = treatment_only(json.load(f))\n",
    "        encoder = re_encode(records)\n",
    "        data.append(records)\n",
    "        encoders.append(encoder)\n",
    "    num_classes =  max([r['enc_sirna'] for r in data[0]]) + 1\n",
    "    return {'data': data, 'encoders': encoders, 'num_classes': num_classes}\n",
    "\n",
    "class RxRxDataset(Dataset):\n",
    "    def __init__(self, items, num_classes, onehot=True, label_smoothing=None,\n",
    "                 features_key='images', targets_key='enc_sirna',\n",
    "                 channels_mode='six', drop_meta=True, open_fn=PIL.Image.open, tr=None):\n",
    "        \n",
    "        assert channels_mode in ('six', 'rgb')\n",
    "        super().__init__()      \n",
    "        self.items = items\n",
    "        self.onehot = onehot\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.features_key = features_key\n",
    "        self.targets_key = targets_key\n",
    "        self.channels_mode = channels_mode\n",
    "        self.drop_meta = drop_meta\n",
    "        self.num_classes = num_classes\n",
    "        self.open_fn = open_fn\n",
    "        self.tr = tr\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.items[index].copy()\n",
    "        bunch = sorted(item.pop(self.features_key), key=itemgetter(0))\n",
    "        channels = OrderedDict()\n",
    "        if self.channels_mode == 'six':\n",
    "            for i, filename in bunch:\n",
    "                channels[f'chan_{i}'] = np.array(self.open_fn(filename))\n",
    "        elif self.channels_mode == 'rgb':\n",
    "            img = np.stack([self.open_fn(filename) for filename in bunch])\n",
    "            img = rio.convert_tensor_to_rgb(img)\n",
    "            for i in range(3):\n",
    "                channes[f'chan_{i}'] = img[i,:,:]\n",
    "        y = item[self.targets_key]\n",
    "        if self.drop_meta:\n",
    "            sample = item\n",
    "            sample['features'] = channels\n",
    "            sample['targets'] = y\n",
    "        else:\n",
    "            sample = dict(features=channels, targets=y)\n",
    "        if self.onehot:\n",
    "            y_enc = get_one_hot(\n",
    "                y, smoothing=self.label_smoothing,\n",
    "                num_classes=self.num_classes)\n",
    "            sample['targets_one_hot'] = y_enc\n",
    "        return sample\n",
    "    \n",
    "    def join_channels(self, x):\n",
    "        return np.stack([channel for _, channel in x['features'].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def augment(x, tr):\n",
    "    main, *rest = list(x['features'].values())\n",
    "    aug_input = dict(image=main)\n",
    "    aug_input.update({f'image{i}': image for i, image in enumerate(rest)})\n",
    "    augmented = tr(**aug_input)\n",
    "    unpacked = [augmented['image']] + [augmented[f'image{i}'] for i in range(5)]\n",
    "    sample = np.stack(unpacked)\n",
    "    return sample\n",
    "\n",
    "class Augmented(Dataset):\n",
    "    def __init__(self, ds, tr):\n",
    "        self.ds = ds\n",
    "        self.tr = tr\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.ds[index]\n",
    "        aug_img = augment(sample, self.tr)\n",
    "        sample['features'] = aug_img\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VisdomCallback(Callback):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 username='username', password='password', \n",
    "                 host='0.0.0.0', port=9090, use_env_creds=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            username (str): Visdom server username.\n",
    "            password (str): Visdom server password.\n",
    "            host (str): Visdom server address.\n",
    "            port (int): Visdom server port.\n",
    "            use_env_creds (bool): If True, then ignore credentials\n",
    "                passed as __init__ parameters and use Visdom \n",
    "                environment variables instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        if use_env_creds:\n",
    "            username = os.environ['VISDOM_USERNAME']\n",
    "            password = os.environ['VISDOM_PASSWORD']\n",
    "            \n",
    "        self.vis = Visdom(\n",
    "            username=username, password=password,\n",
    "            server=host, port=port)        \n",
    "        \n",
    "class BatchMetricsPlotCallback(VisdomCallback):\n",
    "    def on_batch_end(self, state):\n",
    "        for k, v in state.metrics.batch_values.items():\n",
    "            self.vis.line(X=[state.step], Y=[v], win=k, name=k, \n",
    "                          update='append', opts=dict(title=k))       \n",
    "\n",
    "class EpochMetricsPlotCallback(VisdomCallback):\n",
    "    def on_epoch_end(self, state):\n",
    "        for k, v in state.metrics.batch_values.items():\n",
    "            self.vis.line(X=[state.step], Y=[v], win=k, name=k,\n",
    "                          update='append', opts=dict(title=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, num_classes, pretrained='imagenet'):\n",
    "    model_fn = pretrainedmodels.__dict__[model_name]\n",
    "    model = model_fn(num_classes=1000, pretrained=pretrained)\n",
    "    dim_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Sequential()\n",
    "    conv1 = model.conv1\n",
    "    new_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)\n",
    "    new_conv.weight.data[:,0:3,:] = conv1.weight.data.clone()\n",
    "    new_conv.weight.data[:,3:6,:] = conv1.weight.data.clone()\n",
    "    model.conv1 = new_conv\n",
    "    del conv1\n",
    "    return model\n",
    "    \n",
    "def freeze_all(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfreeze_head(model):\n",
    "    for param in model.last_linear.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def get_layer(model, key):\n",
    "    \"\"\"Gets model layer using a key.\n",
    "    \n",
    "    The key could be hierarchical, like first.second.third where\n",
    "    each dot separates hierarchy level.\n",
    "    \"\"\"\n",
    "    parts = key.split('.')\n",
    "    block = model\n",
    "    for part in parts:\n",
    "        block = getattr(block, part)\n",
    "    return block\n",
    "\n",
    "def unfreeze_layers(model, names):\n",
    "    for name in names:\n",
    "        layer = get_layer(model, name)\n",
    "        print(f'Unfreezing layer {name}')\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "data_dict = load_data()\n",
    "trn_rec, tst_rec = data_dict['data']\n",
    "num_classes = data_dict['num_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "cell_types = defaultdict(list)\n",
    "for record in trn_rec:\n",
    "    cell_types[record['cell_type']].append(record)\n",
    "\n",
    "train, valid = [], []\n",
    "for ct, records in cell_types.items():\n",
    "    print(f'Splitting train/test for type: {ct}')\n",
    "    labels = np.array([r['enc_sirna'] for r in records])\n",
    "    ct_train, ct_valid = train_test_split(records, stratify=labels, test_size=0.2)\n",
    "    train.extend(ct_train)\n",
    "    valid.extend(ct_valid)\n",
    "    print(f'... split counts: {len(ct_train)}/{len(ct_valid)} [total: {len(records)}]')\n",
    "\n",
    "transforms = T.Compose(\n",
    "    additional_targets={f'image{i}': 'image' for i in range(5)},\n",
    "    transforms=[\n",
    "        T.Resize(224, 224), \n",
    "        T.VerticalFlip(p=0.25),\n",
    "        T.HorizontalFlip(p=0.25),\n",
    "        T.Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 650\n",
    "trn_ds = Augmented(RxRxDataset(train, num_classes, onehot=True), tr=transforms)\n",
    "val_ds = Augmented(RxRxDataset(valid, num_classes, onehot=True), tr=transforms)\n",
    "loaders = dict(\n",
    "    train=DataLoader(trn_ds, batch_size=batch_size, num_workers=12, shuffle=True),\n",
    "    valid=DataLoader(val_ds, batch_size=batch_size, num_workers=12, shuffle=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        y_onehot = torch.eye(x.size(-1))[y.detach().clone()]\n",
    "        logits = F.softmax(x.cpu(), dim=1).clamp(self.eps, 1 - self.eps)\n",
    "        ce_loss = -1 * y_onehot * torch.log(logits)\n",
    "        focal_loss = ce_loss * (1 - logits)**self.gamma\n",
    "        return focal_loss.to(x.device).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "epochs = 50\n",
    "lrs = 1e-5, 1e-4, 1e-3\n",
    "model = get_model('resnet50', num_classes)\n",
    "freeze_all(model)\n",
    "unfreeze_layers(model, ['layer4', 'last_linear'])\n",
    "conv_lr, layer_lr, head_lr = lrs\n",
    "opt = torch.optim.AdamW(\n",
    "    params=[\n",
    "        # {'params': model.conv1.parameters(), 'lr': conv_lr},\n",
    "        {'params': model.layer4.parameters(), 'lr': layer_lr},\n",
    "        {'params': model.last_linear.parameters(), 'lr': head_lr}\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "sched = CosineAnnealingWarmRestarts(opt, T_0=len(loaders['train']), T_mult=2, eta_min=1e-6)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "runner = SupervisedRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "runner.train(\n",
    "    model=model,\n",
    "    num_epochs=epochs,\n",
    "    criterion=loss_fn,\n",
    "    optimizer=opt,\n",
    "    scheduler=sched,\n",
    "    logdir='/tmp/cells_split/',\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_classes),\n",
    "        BatchMetricsPlotCallback(use_env_creds=True),\n",
    "        EpochMetricsPlotCallback(use_env_creds=True)\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet34_224_ce_1linear_adamw_coswr_difflr_e25_bs650',\n",
       " 'resnet34_224_focal_1linear_adamw_coswr_difflr_e25_bs650',\n",
       " 'resnet101_224_ce_1linear_adamw_coswr_difflr_e25_bs650']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from basedir import NUM_CLASSES\n",
    "logdir = os.path.expanduser('~/logs/protein/runs')\n",
    "os.listdir(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(\n",
    "    logdir,\n",
    "    # 'resnet34_224_ce_1linear_adamw_coswr_difflr_e25_bs650',\n",
    "    'resnet34_224_focal_1linear_adamw_coswr_difflr_e25_bs650',\n",
    "    'checkpoints',\n",
    "    'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_with_simple_custom_head(model_name, num_classes, pretrained='imagenet'):\n",
    "    model_fn = pretrainedmodels.__dict__[model_name]\n",
    "    model = model_fn(num_classes=1000, pretrained=pretrained)\n",
    "    dim_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(dim_feats, num_classes)\n",
    "    conv1 = model.conv1\n",
    "    new_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)\n",
    "    new_conv.weight.data[:, 0:3, :] = conv1.weight.data.clone()\n",
    "    new_conv.weight.data[:, 3:6, :] = conv1.weight.data.clone()\n",
    "    model.conv1 = new_conv\n",
    "    del conv1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda:1')\n",
    "model = get_model_with_simple_custom_head('resnet34', NUM_CLASSES)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(dev)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data()\n",
    "_, tst_rec = data_dict['data']\n",
    "tst_ds = Augmented(\n",
    "    ds=RxRxDataset(tst_rec, NUM_CLASSES, onehot=False), \n",
    "    tr=T.Compose(\n",
    "        additional_targets={f'image{i}': 'image' for i in range(5)},\n",
    "        transforms=[\n",
    "            T.Resize(224, 224),\n",
    "            T.Normalize((0.5,), (0.5),)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "tst_dl = DataLoader(tst_ds, batch_size=1024, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f9fcb1266247dba042a75e4ba3be12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for batch in tqdm(tst_dl):\n",
    "    out = model(batch['features'].to(dev))\n",
    "    y = out.softmax(dim=1)\n",
    "    preds.extend(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(num_classes)\n",
    "checkpoints = []\n",
    "best = '/tmp/cells_split/checkpoints/train.2.pth'\n",
    "checkpoint = torch.load(best)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model.to('cuda:0')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data()\n",
    "_, tst_rec = data_dict['data']\n",
    "tst_ds = Augmented(RxRxDataset(tst_rec, NUM_CLASSES, onehot=False), tr=T.Compose([\n",
    "    T.Normalize((0.5,), (0.5),)\n",
    "]))\n",
    "tst_dl = DataLoader(tst_ds, batch_size=512, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce94be1b6dc34a72a6266f09bcf1d689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=311), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Process Process-11:\n",
      "Process Process-5:\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1ca4ed5d08>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-08724a4ea5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 325, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 265, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/util.py\", line 189, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/ck/anaconda3/envs/fastai_10/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for batch in tqdm(tst_dl):\n",
    "    out = model(batch['features'].to(dev))\n",
    "    y = out.softmax(dim=1)\n",
    "    preds.extend(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_into_test(filename):\n",
    "    basename, _ = os.path.splitext(os.path.basename(filename))\n",
    "    sirna = int(basename.split('_')[-1])\n",
    "    return sirna == 0\n",
    "\n",
    "filenames = sorted(list_files('~/data/protein/tmp/test'))\n",
    "filenames = [fn for fn in filenames if include_into_test(fn)]\n",
    "\n",
    "# odd\n",
    "site1 = []\n",
    "for filename, pred in list(zip(filenames, preds))[::2]:\n",
    "    basename, _ = os.path.splitext(os.path.basename(filename))\n",
    "    sirna = int(basename.split('_')[-1])\n",
    "    if sirna != 0: \n",
    "        continue\n",
    "    site1.append(pred)\n",
    "    \n",
    "# even\n",
    "site2 = []\n",
    "for filename, pred in list(zip(filenames, preds))[1::2]:\n",
    "    basename, _ = os.path.splitext(os.path.basename(filename))\n",
    "    sirna = int(basename.split('_')[-1])\n",
    "    if sirna != 0: \n",
    "        continue\n",
    "    site2.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19897])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(site1)\n",
    "t2 = torch.tensor(site2)\n",
    "avg_pred = ((t1 + t2)/2).argmax(dim=1)\n",
    "print(avg_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submit.csv' target='_blank'>submit.csv</a><br>"
      ],
      "text/plain": [
       "/home/ck/code/tasks/protein/submit.csv"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv('/home/ck/data/protein/sample_submission.csv')\n",
    "sample['sirna'] = avg_pred.tolist()\n",
    "sample.to_csv('submit.csv', index=False)\n",
    "from IPython.display import FileLink\n",
    "FileLink('submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: 06_even_more_simple_training.ipynb -> even_more_simple_training.py\r\n",
      "1 notebook(s) exported into folder: .\r\n"
     ]
    }
   ],
   "source": [
    "!python -m jupytools export -nb \"06_even_more_simple_training.ipynb\" -o ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "from os.path import dirname, join\n",
    "from functools import reduce\n",
    "from pdb import set_trace\n",
    "\n",
    "import cv2 as cv\n",
    "import jupytools\n",
    "import jupytools.syspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pretrainedmodels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from visdom import Visdom\n",
    "\n",
    "jupytools.syspath.add(join(dirname(os.getcwd()), 'protein_project'))\n",
    "jupytools.syspath.add('rxrx1-utils')\n",
    "if jupytools.is_notebook():\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "else:\n",
    "    from tqdm import tqdm as tdqm\n",
    "    \n",
    "import contextlib, io\n",
    "with contextlib.redirect_stderr(io.StringIO()):\n",
    "    # from augmentation import Augmented, multichannel_norm\n",
    "    from basedir import ROOT, NUM_CLASSES\n",
    "    from catalyst_visdom import BatchMetricsPlotCallback, EpochMetricsPlotCallback\n",
    "    from dataset import load_data, build_stats_index, RxRxDataset\n",
    "    from model.utils import freeze_all, unfreeze_layers\n",
    "    from visual import rgb, six, show_1, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "data_dict = load_data(filenames=['train.json', 'test.json'])\n",
    "train_records, test_records = data_dict['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bernoulli(p): return int(np.random.binomial(1, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseTransform:\n",
    "    def __init__(self, apply_always=False, p=1.0, key='features'):\n",
    "        self.apply_always = apply_always\n",
    "        self.p = p\n",
    "        self.key = key\n",
    "    def __call__(self, x):\n",
    "        if self.apply_always or bernoulli(self.p):\n",
    "            return self.apply(x)\n",
    "        return x\n",
    "    def apply(self, x):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnconditionalTransform(BaseTransform):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(apply_always=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Compose(UnconditionalTransform):\n",
    "    def __init__(self, transforms):\n",
    "        super().__init__(apply_always=True)\n",
    "        self.transforms = transforms\n",
    "    def apply(self, dataset_record):\n",
    "        return reduce(lambda x, f: f(x), self.transforms, dataset_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class JoinChannels(UnconditionalTransform):\n",
    "    def apply(self, record):\n",
    "        record['features'] = np.stack(list(record['features'].values()))\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Resize(UnconditionalTransform):\n",
    "    def __init__(self, sz):\n",
    "        super().__init__()\n",
    "        self.sz = (sz, sz) if isinstance(sz, int) else sz\n",
    "    def apply(self, record):\n",
    "        record['features'] = cv.resize(record['features'], self.sz) \n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToFloat(UnconditionalTransform):\n",
    "    def apply(self, record):\n",
    "        record['features'] = record['features'].astype(np.float32)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SwapChannels(UnconditionalTransform):\n",
    "    def __init__(self, new_order):\n",
    "        super().__init__()\n",
    "        self.new_order = new_order\n",
    "    def apply(self, record):\n",
    "        record['features'] = record['features'].transpose(*self.new_order)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VerticalFlip(BaseTransform):\n",
    "    def apply(self, record): \n",
    "        record['features'] = cv.flip(record['features'], 0)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HorizontalFlip(BaseTransform):\n",
    "    def apply(self, record): \n",
    "        record['features'] = cv.flip(record['features'], 1)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PixelStatsNorm(UnconditionalTransform):\n",
    "    def __init__(self, stats):\n",
    "        super().__init__()\n",
    "        self.stats = stats\n",
    "    def apply(self, record):\n",
    "        keys = [(record['id_code'], i+1, record['site']) for i in range(6)]\n",
    "        mean = np.stack([self.stats[key]['mean'] for key in keys]).astype(np.float32)\n",
    "        std = np.stack([self.stats[key]['std'] for key in keys]).astype(np.float32)\n",
    "        mean, std = [arr.reshape(len(arr), 1, 1) for arr in (mean, std)]\n",
    "        norm = (record['features'] - mean)/(std + 1e-8)\n",
    "        record['features'] = norm\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Rescale(UnconditionalTransform):\n",
    "    def __init__(self, factor=1./255):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "    def apply(self, record):\n",
    "        record['features'] = record['features'] * self.factor\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Augmented(Dataset):\n",
    "    def __init__(self, ds, tr):\n",
    "        self.ds = ds\n",
    "        self.tr = tr\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, index):\n",
    "        return self.tr(self.ds[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition and Normalization Stats Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "stats = build_stats_index(ROOT/'pixel_stats.csv')\n",
    "\n",
    "train_transform = Compose([\n",
    "    JoinChannels(),\n",
    "    SwapChannels((1, 2, 0)),\n",
    "    Resize(224),\n",
    "    ToFloat(),\n",
    "    VerticalFlip(p=0.25),\n",
    "    HorizontalFlip(p=0.25),\n",
    "    SwapChannels((2, 0, 1)),\n",
    "    # PixelStatsNorm(stats)\n",
    "    Rescale()\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    JoinChannels(),\n",
    "    SwapChannels((1, 2, 0)),\n",
    "    Resize(224),\n",
    "    ToFloat(),\n",
    "    SwapChannels((2, 0, 1)),\n",
    "    # PixelStatsNorm(stats)\n",
    "    Rescale()\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    JoinChannels(),\n",
    "    SwapChannels((1, 2, 0)),\n",
    "    Resize(224),\n",
    "    ToFloat(),\n",
    "    SwapChannels((2, 0, 1)),\n",
    "    # PixelStatsNorm(stats)\n",
    "    Rescale()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(stats)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split(records):\n",
    "    experiments = np.unique([r['experiment'] for r in records])\n",
    "    cell_types = np.unique([r['cell_type'] for r in records])\n",
    "    holdouts = []\n",
    "    for ct in cell_types:\n",
    "        holdout = np.random.choice(experiments[\n",
    "            [exp.startswith(ct) for exp in experiments]])\n",
    "        holdouts.append(holdout)\n",
    "\n",
    "    train_exp, valid_exp = [exp for exp in experiments if exp not in holdouts], holdouts\n",
    "    #print(f'Train experiments: {train_exp}')\n",
    "    #print(f'Valid experiments: {valid_exp}')\n",
    "    train = [r for r in records if r['experiment'] in train_exp]\n",
    "    valid = [r for r in records if r['experiment'] in valid_exp]\n",
    "    #print(f'Train size: {len(train)}, valid size: {len(valid)}')\n",
    "    \n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_loaders(data, batch_size, n_cpu=12, with_test=False):\n",
    "    train_records, valid_records = split(data)\n",
    "\n",
    "    trn_ds = RxRxDataset(train_records, num_classes=NUM_CLASSES)\n",
    "    aug_trn_ds = Augmented(trn_ds, train_transform)\n",
    "\n",
    "    val_ds = RxRxDataset(valid_records, num_classes=NUM_CLASSES)\n",
    "    aug_val_ds = Augmented(val_ds, valid_transform)\n",
    "\n",
    "    loaders = OrderedDict([\n",
    "        ('train', DataLoader(\n",
    "            aug_trn_ds, batch_size=batch_size, \n",
    "            shuffle=True, num_workers=n_cpu)),\n",
    "        ('valid', DataLoader(\n",
    "            aug_val_ds, batch_size=batch_size, \n",
    "            shuffle=False, num_workers=n_cpu)),\n",
    "    ])\n",
    "    \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from catalyst.contrib.modules import GlobalConcatPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_resnet_model(size, num_classes):\n",
    "    model_name = f'resnet{size}'\n",
    "    model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
    "    feat_dim = model.last_linear.in_features\n",
    "    model.last_linear = torch.nn.Linear(feat_dim, num_classes)\n",
    "    conv1 = model.conv1.weight\n",
    "    new_conv = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    with torch.no_grad():\n",
    "        new_conv.weight[:,:] = torch.stack([torch.mean(conv1, 1)]*6, dim=1)\n",
    "    model.conv1 = new_conv\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RollingLoss:\n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "        self.prev = 0\n",
    "    def __call__(self, curr, batch_no):\n",
    "        a = self.smooth\n",
    "        avg_loss = a*self.prev + (1 - a)*curr\n",
    "        debias_loss = avg_loss/(1 - a**batch_no)\n",
    "        self.prev = avg_loss\n",
    "        return debias_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def freeze_all(model):\n",
    "    for child in model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_all(model):\n",
    "    print('Unfreezing all model layers')\n",
    "    for child in model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_layers(model, names):\n",
    "    print('Unfreezing layers:', names)\n",
    "    for name, child in model.named_children():\n",
    "        if name in names:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "model = create_resnet_model(size=34, num_classes=NUM_CLASSES)\n",
    "freeze_all(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "epochs = 1000\n",
    "patience = 10\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.05)\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.3, patience=10, mode='max')\n",
    "model = model.to(device)\n",
    "rolling_loss = dict(train=RollingLoss(), valid=RollingLoss())\n",
    "steps = dict(train=0, valid=0)\n",
    "\n",
    "trials = 0\n",
    "best_metric = -np.inf\n",
    "history = []\n",
    "stop = False\n",
    "\n",
    "vis = Visdom(server='0.0.0.0', port=9090,\n",
    "             username=os.environ['VISDOM_USERNAME'],\n",
    "             password=os.environ['VISDOM_PASSWORD'])\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'Epoch [{epoch}/{epochs}]')\n",
    "    \n",
    "    if epoch == 1:\n",
    "        bs = 800\n",
    "        unfreeze_layers(model, ['last_linear'])\n",
    "        loaders = create_loaders(train_records, batch_size=bs)\n",
    "    elif epoch == 2:\n",
    "        bs = 650\n",
    "        unfreeze_layers(model, ['layer4'])\n",
    "        loaders = create_loaders(train_records, batch_size=bs)\n",
    "    elif epoch == 3:\n",
    "        unfreeze_layers(model, ['layer3'])\n",
    "    elif epoch == 5:\n",
    "        bs = 512\n",
    "        unfreeze_layers(model, ['layer2'])\n",
    "        loaders = create_loaders(train_records, batch_size=bs)\n",
    "    elif epoch == 7:\n",
    "        bs = 256\n",
    "        unfreeze_all(model)\n",
    "        loaders = create_loaders(train_records, batch_size=bs)\n",
    "        \n",
    "    iteration = dict(epoch=epoch, train_loss=list(), valid_loss=list())\n",
    "    print(f\"Current learning rate: {opt.param_groups[0]['lr']:.2E}\")\n",
    "    \n",
    "    for name, loader in loaders.items():\n",
    "        is_training = name == 'train'\n",
    "        count = 0\n",
    "        metric = 0.0\n",
    "        \n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            for batch in loader:\n",
    "                steps[name] += 1\n",
    "                opt.zero_grad()\n",
    "                x = batch['features'].to(device)\n",
    "                y = batch['targets'].to(device)\n",
    "                out = model(x)\n",
    "                \n",
    "                if is_training:\n",
    "                    loss = loss_fn(out, y)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                \n",
    "                avg_loss = rolling_loss[name](loss.item(), steps[name])\n",
    "                iteration[f'{name}_loss'].append(avg_loss)\n",
    "                y_pred = out.softmax(dim=1).argmax(dim=1)\n",
    "                acc = (y_pred == y).float().mean().item()\n",
    "                metric += acc\n",
    "                count += len(batch)\n",
    "                vis.line(X=[steps[name]], Y=[avg_loss], name=f'{name}_loss', \n",
    "                         win=f'{name}_loss', update='append', \n",
    "                         opts=dict(title=f'Running Loss [{name}]'))\n",
    "        \n",
    "        metric /= count\n",
    "        iteration[f'{name}_acc'] = metric\n",
    "        vis.line(X=[epoch], Y=[avg_loss], name=f'{name}', win='avg_loss',\n",
    "                 update='append', opts=dict(title='Average Epoch Loss'))\n",
    "        vis.line(X=[epoch], Y=[metric], name=f'{name}', win='accuracy', \n",
    "                 update='append', opts=dict(title=f'Accuracy'))\n",
    "        \n",
    "        last_loss = iteration[f'{name}_loss'][-1]\n",
    "        \n",
    "        print(f'{name} metrics: accuracy={metric:2.3%}, loss={last_loss:.4f}')\n",
    "          \n",
    "        if not is_training:\n",
    "            sched.step(metric)\n",
    "            if metric > best_metric:\n",
    "                trials = 0\n",
    "                best_metric = metric\n",
    "                print('Score improved!')\n",
    "                torch.save(model.state_dict(), f'train.{epoch}.pth')\n",
    "            else:\n",
    "                trials += 1\n",
    "                if trials >= patience:\n",
    "                    stop = True\n",
    "                    break\n",
    "    \n",
    "    history.append(iteration)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    \n",
    "    if stop:\n",
    "        print(f'Early stopping on epoch: {epoch}')\n",
    "        break\n",
    "\n",
    "torch.save(history, 'history.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "class TestDataset:\n",
    "    def __init__(self, records, open_fn=PIL.Image.open):\n",
    "        self.records = records\n",
    "        self.open_fn = open_fn\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, index):\n",
    "        record = self.records[index]\n",
    "        files = record['images']\n",
    "        features = OrderedDict([(i, self.open_fn(filename)) for i, filename in files])\n",
    "        return {'features': features, 'site': record['site'], 'id_code': record['id_code']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "model = create_resnet_model(size=34, num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "freeze_all(model)\n",
    "model.load_state_dict(torch.load('train.24.pth'))\n",
    "model.eval()\n",
    "\n",
    "def load_test_data(filename='test.json'):\n",
    "    with open(filename) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "test_records = load_test_data()\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    Augmented(\n",
    "        ds=TestDataset(test_records),\n",
    "        tr=test_transform\n",
    "    ),\n",
    "    batch_size=512,\n",
    "    shuffle=False, \n",
    "    num_workers=12\n",
    ")\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dl):\n",
    "        out = model(batch['features'].to(device))\n",
    "        y = out.softmax(dim=1)\n",
    "        preds.extend(list(zip(batch['id_code'], batch['site'].tolist(), y.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(preds):\n",
    "    site1, site2 = [], []\n",
    "    for id_code, site, probs in preds:\n",
    "        agg = site1 if site == 1 else site2\n",
    "        agg.append(probs)\n",
    "    t1 = torch.tensor(site1)\n",
    "    t2 = torch.tensor(site2)\n",
    "    avg_pred = ((t1 + t2)/2).argmax(dim=1)\n",
    "    id_codes = []\n",
    "    for id_code, _, _ in preds:\n",
    "        if id_code in id_codes:\n",
    "            continue\n",
    "        id_codes.append(id_code)\n",
    "    return pd.DataFrame({'id_code': id_codes, 'sirna': avg_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site1, site2 = [], []\n",
    "for id_code, site, probs in preds:\n",
    "    agg = site1 if site == 1 else site2\n",
    "    agg.append(probs)\n",
    "t1 = torch.tensor(site1)\n",
    "t2 = torch.tensor(site2)\n",
    "avg_pred = ((t1 + t2)/2).argmax(dim=1)\n",
    "id_codes = []\n",
    "for id_code, _, _ in preds:\n",
    "    if id_code in id_codes:\n",
    "        continue\n",
    "    id_codes.append(id_code)\n",
    "preds_df = pd.DataFrame({'id_code': id_codes, 'sirna': avg_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(preds)//2 == len(pd.read_csv('/home/ck/data/protein/sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(list_files('~/data/protein/tmp/test'))\n",
    "\n",
    "# odd\n",
    "site1 = []\n",
    "for filename, pred in list(zip(filenames, preds))[::2]:\n",
    "    basename, _ = os.path.splitext(os.path.basename(filename))\n",
    "    sirna = int(basename.split('_')[-1])\n",
    "    if sirna != 0: \n",
    "        continue\n",
    "    site1.append(pred)\n",
    "    \n",
    "# even\n",
    "site2 = []\n",
    "for filename, pred in list(zip(filenames, preds))[1::2]:\n",
    "    basename, _ = os.path.splitext(os.path.basename(filename))\n",
    "    sirna = int(basename.split('_')[-1])\n",
    "    if sirna != 0: \n",
    "        continue\n",
    "    site2.append(pred)\n",
    "    \n",
    "t1 = torch.tensor(site1)\n",
    "t2 = torch.tensor(site2)\n",
    "avg_pred = ((t1 + t2)/2).argmax(dim=1)\n",
    "print(avg_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('/home/ck/data/protein/sample_submission.csv')\n",
    "sample['sirna'] = avg_pred.tolist()\n",
    "sample.to_csv('submit.csv', index=False)\n",
    "from IPython.display import FileLink\n",
    "FileLink('submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf *.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# -----------------------------------------
# THIS FILE WAS AUTOGENERATED! DO NOT EDIT!
# -----------------------------------------
# file to edit: 02_six_channels.ipynb

from collections import OrderedDict
import glob
import os
from operator import itemgetter
import json
from multiprocessing import cpu_count
import re
import sys
from pdb import set_trace
from pprint import pprint as pp
import warnings
warnings.filterwarnings('ignore')

import ancli
from imageio import imread
from jupytools import auto_set_trace, is_notebook
import numpy as np
import pandas as pd
import PIL.Image

from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from torch.utils.data import Dataset, DataLoader
import torchvision
import torchvision.transforms as T
from torchvision.transforms.functional import to_tensor
from tqdm import tqdm
import pretrainedmodels
from visdom import Visdom

from catalyst.contrib.schedulers import OneCycleLR
from catalyst.data.dataset import ListDataset
from catalyst.dl.callbacks import AccuracyCallback, AUCCallback, F1ScoreCallback
from catalyst.dl.runner import SupervisedRunner
from catalyst.utils import get_one_hot

try:
    extended
except NameError:
    sys.path.insert(0, 'rxrx1-utils')
    import rxrx.io as rio

from basedir import ROOT, TRAIN, TEST, SAMPLE, NUM_CLASSES


seed = 1
dev_id = 0
device = torch.device(dev_id)
set_trace = auto_set_trace()
# os.environ['CUDA_VISIBLE_DEVICES'] = f'{dev_id}'


def list_files(folder):
    dirname = os.path.expanduser(folder)
    return [os.path.join(dirname, x) for x in os.listdir(dirname)]


def load_data():
    content = []
    for filename in ('train', 'test'):
        with open(f'{filename}.json') as f:
            content.append(json.load(f))
    return content


class RxRxDataset(Dataset):

    def __init__(self, items, onehot=True,
                 label_smoothing=None,
                 open_fn=PIL.Image.open, tr=None):

        super().__init__()
        
        self.onehot = onehot
        self.label_smoothing = label_smoothing
        self.open_fn = open_fn
        self.tr = tr
        #self.targets = [item['sirna'] for item in items]
        #self.images = [item['images'] for item in items]
        self.items = items
        self.num_classes = len(np.unique(self.targets))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, index):
        bunch = self.items[index]['images']
        sample = self.items[index].copy()
        channels = []
        for i, filename in sorted(bunch, key=itemgetter(0)):
            img = self.open_fn(filename)
            img = img if self.tr is None else self.tr(img)
            channels.append(img)
        sample = torch.cat(channels, dim=0)
        y = self.targets[index]
        sample.update(dict(features=sample, targets=y))
        if self.onehot:
            y_enc = get_one_hot(
                y, smoothing=self.label_smoothing,
                num_classes=self.num_classes)
            sample['targets_one_hot'] = y_enc
        from pdb import set_trace
        set_trace()
        return sample


class _Split:

    def __init__(self, test_size=0.1, seed=1):
        self.test_size = test_size
        self.seed = seed

    def __call__(self, *args, **kwargs):
        raise NotImplementedError()


class TargetSplit(_Split):
    def __call__(self, records):
        ys = [r['sirna'] for r in records]
        train, valid = train_test_split(
            records, stratify=ys,
            test_size=self.test_size,
            random_state=self.seed)
        return train, valid


class DataBunch:

    def __init__(self, input_size=224, stats=None, aug_raw=None, aug_resized=None):
        self.input_size = input_size
        self.stats = stats or {'mean': (0.5,), 'std': (0.5,)}
        self.aug_raw = aug_raw
        self.aug_resized = aug_resized

    def create(self, train_records, test_records, **kwargs):
        trn_dl, val_dl = self.create_train_valid(train_records, **kwargs)
        tst_dl = self.create_test(test_records, **kwargs)
        loaders = OrderedDict([('train', trn_dl), ('valid', val_dl), ('test', tst_dl)])
        return loaders

    def create_train_valid(self, records, batch_size=4, split=None, test_size=0.1):
        split = split or TargetSplit(test_size)
        train, valid = split(records)
        trn_ds = RxRxDataset(train, tr=self.build_tr(True))
        val_ds = RxRxDataset(valid, tr=self.build_tr(False))
        trn_dl = self.build_dl(trn_ds, bs=batch_size)
        val_dl = self.build_dl(val_ds, bs=batch_size, shuffle=False)
        return trn_dl, val_dl

    def create_test(self, records, batch_size=4):
        tst_ds = RxRxDataset(records, tr=self.build_tr(False))
        tst_dl = self.build_dl(tst_ds, bs=batch_size, shuffle=False)
        return tst_dl

    def build_tr(self, train=True):
        tr = []
        if train:
            if self.aug_raw is not None:
                tr.extend(self.aug_raw)
        tr.append(T.Resize(self.input_size))
        if train:
            if self.aug_resized is not None:
                tr.extend(self.aug_resized)
        tr.append(T.ToTensor())
        tr.append(T.Normalize(self.stats['mean'], self.stats['std']))
        return T.Compose(tr)

    def build_dl(self, ds, bs, shuffle=True):
        return DataLoader(ds, shuffle=shuffle, batch_size=bs, num_workers=cpu_count())


model_name = 'resnet50'


def get_model(model_name, num_classes, pretrained='imagenet'):
    model_fn = pretrainedmodels.__dict__[model_name]
    model = model_fn(num_classes=1000, pretrained=pretrained)
    dim_feats = model.last_linear.in_features
    model.last_linear = nn.Linear(dim_feats, num_classes)
    new_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)
    new_conv.weight.data[:,0:3,:] = model.conv1.weight.data.clone()
    new_conv.weight.data[:,3:6,:] = model.conv1.weight.data.clone()
    model.conv1 = new_conv
    return model


class RollingLoss:
    def __init__(self, smooth=0.98):
        self.smooth = smooth
        self.prev = 0
    def __call__(self, curr, batch_no):
        a = self.smooth
        avg_loss = a*self.prev + (1 - a)*curr
        debias_loss = avg_loss/(1 - a**batch_no)
        self.prev = avg_loss
        return debias_loss


def freeze_model(model):
    for param in model.parameters():
        param.requires_grad = False


def get_layer(model, key):
    """Gets model layer using a key.

    The key could be hierarchical, like first.second.third where
    each dot separates hierarchy level.
    """
    parts = key.split('.')
    block = model
    for part in parts:
        block = getattr(block, part)
    return block


def unfreeze_layers(model, names):
    for name in names:
        layer = get_layer(model, name)
        print(f'Unfreezing layer {name}')
        for param in layer.parameters():
            param.requires_grad = True


def train(epochs: int=1,
          batch_size: int=800,
          model_name: str='resnet34',
          logdir: str='/tmp/loops/',
          lrs: tuple=(1e-4, 1e-3, 5e-3),
          eta_min: float=1e-6,
          dev_id: int=1,
          visdom_host: str='0.0.0.0',
          visdom_port: int=9001):

    vis = Visdom(server=visdom_host, port=visdom_port,
                 username=os.environ['VISDOM_USERNAME'],
                 password=os.environ['VISDOM_PASSWORD'])

    experiment_id = f'{model_name}_e{epochs}_b{batch_size}'
    device = torch.device(f'cuda:{dev_id}')
    # dataset = create_data_loaders(*load_data(), batch_size=batch_size)
    dataset = DataBunch().create(*load_data(), batch_size=batch_size)
    model = get_model(model_name, NUM_CLASSES).to(device)
    freeze_model(model)
    unfreeze_layers(model, ['conv1', 'bn1', 'layer4', 'last_linear'])

    loss_fn = nn.CrossEntropyLoss()
    conv, layer, head = lrs
    opt = torch.optim.AdamW([
        {'params': model.conv1.parameters(), 'lr': conv},
        {'params': model.layer4.parameters(), 'lr': layer},
        {'params': model.last_linear.parameters(), 'lr': head}
    ], weight_decay=0.01)
    logdir = os.path.join(logdir, experiment_id)
    sched = CosineAnnealingWarmRestarts(
        opt, T_0=len(dataset['train']), T_mult=2, eta_min=eta_min)
    rolling_loss = RollingLoss()
    os.makedirs(logdir, exist_ok=True)
    iteration = 0

    for epoch in range(1, epochs+1):
        trn_dl = dataset['train']
        n = len(trn_dl)

        model.train()
        with tqdm(total=n) as bar:
            for i, batch in enumerate(trn_dl, 1):
                iteration += 1
                if i % 25 == 0:
                    for j, g in enumerate(opt.param_groups):
                        vis.line(X=[iteration], Y=[g['lr']],
                                 win=f'metrics{j}', name=f'lr{j}', update='append')
                bar.set_description(f'[epoch:{epoch}/{epochs}][{i}/{n}]')
                opt.zero_grad()
                x = batch['features'].to(device)
                y = batch['targets'].to(device)
                out = model(x)
                loss = loss_fn(out, y)
                loss.backward()
                avg_loss = rolling_loss(loss.item(), iteration+1)
                opt.step()
                sched.step()
                bar.set_postfix(avg_loss=f'{avg_loss:.3f}')
                bar.update(1)
                vis.line(X=[iteration], Y=[avg_loss],
                         win='loss', name='avg_loss', update='append')

        val_dl = dataset['valid']
        n = len(val_dl)

        model.eval()
        with torch.no_grad():
            matches = []
            with tqdm(total=n) as bar:
                for batch in val_dl:
                    x = batch['features'].to(device)
                    y = batch['targets'].to(device)
                    out = model(x)
                    y_pred = out.softmax(dim=1).argmax(dim=1)
                    matched = (y == y_pred).detach().cpu().numpy().tolist()
                    matches.extend(matched)
                    bar.update(1)
            acc = np.mean(matches)
            vis.line(X=[epoch], Y=[acc], win='acc', name='val_acc', update='append')
            print(f'validation accuracy: {acc:2.2%}')
            acc_str = str(int(round(acc * 10_000, 0)))
            path = os.path.join(logdir, f'train.{epoch}.{acc_str}.pth')
            torch.save(model.state_dict(), path)


if __name__ == '__main__':
    if not is_notebook():
        ancli.make_cli(train)

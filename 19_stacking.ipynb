{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from basedir import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv('stacked/train/densenet121_long_training.csv')\n",
    "f2 = pd.read_csv('stacked/train/densenet121_with_tricks.csv')\n",
    "f3 = pd.read_csv('stacked/train/densenet169.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv(ROOT/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([f.drop(columns='id_code').values for f in (f1, f2, f3)])\n",
    "y = trn_df['sirna'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36515, 3324), (36515,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36515, 3324]) torch.FloatTensor torch.Size([36515]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(X).float()\n",
    "y = torch.tensor(y)\n",
    "print(X.shape, X.type(), y.shape, y.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3324, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.logreg = nn.Linear(1024, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.logreg(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][0] Loss: 7.0111\n",
      "[0][1] Loss: 7.0039\n",
      "[0][2] Loss: 6.9970\n",
      "[0][3] Loss: 6.9851\n",
      "[0][4] Loss: 6.9763\n",
      "[0][5] Loss: 6.9659\n",
      "[0][6] Loss: 6.9517\n",
      "[0][7] Loss: 6.9359\n",
      "[0][8] Loss: 6.9148\n",
      "[0][9] Loss: 6.9015\n",
      "[0][10] Loss: 6.8792\n",
      "[0][11] Loss: 6.8568\n",
      "[0][12] Loss: 6.8362\n",
      "[0][13] Loss: 6.8100\n",
      "[0][14] Loss: 6.7754\n",
      "[0][15] Loss: 6.7296\n",
      "[0][16] Loss: 6.7049\n",
      "[0][17] Loss: 6.6776\n",
      "[0][18] Loss: 6.6192\n",
      "[0][19] Loss: 6.5915\n",
      "[0][20] Loss: 6.5343\n",
      "[0][21] Loss: 6.4677\n",
      "[0][22] Loss: 6.3976\n",
      "[0][23] Loss: 6.3470\n",
      "[0][24] Loss: 6.2673\n",
      "[0][25] Loss: 6.1761\n",
      "[0][26] Loss: 6.1197\n",
      "[0][27] Loss: 6.0255\n",
      "[0][28] Loss: 5.9288\n",
      "[0][29] Loss: 5.8081\n",
      "[0][30] Loss: 5.6807\n",
      "[0][31] Loss: 5.5962\n",
      "[0][32] Loss: 5.4307\n",
      "[0][33] Loss: 5.2785\n",
      "[0][34] Loss: 5.1475\n",
      "[0][35] Loss: 5.0174\n",
      "[1][0] Loss: 4.5792\n",
      "[1][1] Loss: 4.4304\n",
      "[1][2] Loss: 4.1691\n",
      "[1][3] Loss: 3.9974\n",
      "[1][4] Loss: 3.6870\n",
      "[1][5] Loss: 3.5483\n",
      "[1][6] Loss: 3.3617\n",
      "[1][7] Loss: 3.1556\n",
      "[1][8] Loss: 2.8102\n",
      "[1][9] Loss: 2.5237\n",
      "[1][10] Loss: 2.2964\n",
      "[1][11] Loss: 1.9804\n",
      "[1][12] Loss: 1.7060\n",
      "[1][13] Loss: 1.5918\n",
      "[1][14] Loss: 1.3961\n",
      "[1][15] Loss: 1.2770\n",
      "[1][16] Loss: 1.1368\n",
      "[1][17] Loss: 0.9099\n",
      "[1][18] Loss: 0.8696\n",
      "[1][19] Loss: 0.7658\n",
      "[1][20] Loss: 0.7066\n",
      "[1][21] Loss: 0.6180\n",
      "[1][22] Loss: 0.5625\n",
      "[1][23] Loss: 0.5209\n",
      "[1][24] Loss: 0.4326\n",
      "[1][25] Loss: 0.3964\n",
      "[1][26] Loss: 0.4050\n",
      "[1][27] Loss: 0.3614\n",
      "[1][28] Loss: 0.3475\n",
      "[1][29] Loss: 0.3456\n",
      "[1][30] Loss: 0.2923\n",
      "[1][31] Loss: 0.2521\n",
      "[1][32] Loss: 0.2345\n",
      "[1][33] Loss: 0.2255\n",
      "[1][34] Loss: 0.2594\n",
      "[1][35] Loss: 0.2359\n",
      "[2][0] Loss: 0.2511\n",
      "[2][1] Loss: 0.1681\n",
      "[2][2] Loss: 0.2102\n",
      "[2][3] Loss: 0.2194\n",
      "[2][4] Loss: 0.2302\n",
      "[2][5] Loss: 0.1843\n",
      "[2][6] Loss: 0.1502\n",
      "[2][7] Loss: 0.1428\n",
      "[2][8] Loss: 0.1546\n",
      "[2][9] Loss: 0.1799\n",
      "[2][10] Loss: 0.1554\n",
      "[2][11] Loss: 0.1800\n",
      "[2][12] Loss: 0.1739\n",
      "[2][13] Loss: 0.1470\n",
      "[2][14] Loss: 0.1408\n",
      "[2][15] Loss: 0.1710\n",
      "[2][16] Loss: 0.1073\n",
      "[2][17] Loss: 0.1052\n",
      "[2][18] Loss: 0.1473\n",
      "[2][19] Loss: 0.1445\n",
      "[2][20] Loss: 0.1340\n",
      "[2][21] Loss: 0.1469\n",
      "[2][22] Loss: 0.1409\n",
      "[2][23] Loss: 0.1159\n",
      "[2][24] Loss: 0.0924\n",
      "[2][25] Loss: 0.1601\n",
      "[2][26] Loss: 0.1594\n",
      "[2][27] Loss: 0.1320\n",
      "[2][28] Loss: 0.1033\n",
      "[2][29] Loss: 0.1037\n",
      "[2][30] Loss: 0.1321\n",
      "[2][31] Loss: 0.1123\n",
      "[2][32] Loss: 0.1253\n",
      "[2][33] Loss: 0.1161\n",
      "[2][34] Loss: 0.1306\n",
      "[2][35] Loss: 0.1693\n",
      "[3][0] Loss: 0.1040\n",
      "[3][1] Loss: 0.1077\n",
      "[3][2] Loss: 0.0919\n",
      "[3][3] Loss: 0.1093\n",
      "[3][4] Loss: 0.0756\n",
      "[3][5] Loss: 0.0534\n",
      "[3][6] Loss: 0.0992\n",
      "[3][7] Loss: 0.1077\n",
      "[3][8] Loss: 0.1063\n",
      "[3][9] Loss: 0.1349\n",
      "[3][10] Loss: 0.0915\n",
      "[3][11] Loss: 0.1034\n",
      "[3][12] Loss: 0.1203\n",
      "[3][13] Loss: 0.1192\n",
      "[3][14] Loss: 0.1163\n",
      "[3][15] Loss: 0.0960\n",
      "[3][16] Loss: 0.1038\n",
      "[3][17] Loss: 0.0946\n",
      "[3][18] Loss: 0.0887\n",
      "[3][19] Loss: 0.0982\n",
      "[3][20] Loss: 0.1293\n",
      "[3][21] Loss: 0.1157\n",
      "[3][22] Loss: 0.0995\n",
      "[3][23] Loss: 0.0907\n",
      "[3][24] Loss: 0.0845\n",
      "[3][25] Loss: 0.0771\n",
      "[3][26] Loss: 0.0667\n",
      "[3][27] Loss: 0.0960\n",
      "[3][28] Loss: 0.1251\n",
      "[3][29] Loss: 0.1030\n",
      "[3][30] Loss: 0.0888\n",
      "[3][31] Loss: 0.0905\n",
      "[3][32] Loss: 0.0889\n",
      "[3][33] Loss: 0.0882\n",
      "[3][34] Loss: 0.0973\n",
      "[3][35] Loss: 0.0908\n",
      "[4][0] Loss: 0.0578\n",
      "[4][1] Loss: 0.0764\n",
      "[4][2] Loss: 0.0907\n",
      "[4][3] Loss: 0.0541\n",
      "[4][4] Loss: 0.0614\n",
      "[4][5] Loss: 0.0541\n",
      "[4][6] Loss: 0.0677\n",
      "[4][7] Loss: 0.0612\n",
      "[4][8] Loss: 0.0992\n",
      "[4][9] Loss: 0.0693\n",
      "[4][10] Loss: 0.0835\n",
      "[4][11] Loss: 0.0933\n",
      "[4][12] Loss: 0.0772\n",
      "[4][13] Loss: 0.0561\n",
      "[4][14] Loss: 0.1084\n",
      "[4][15] Loss: 0.0764\n",
      "[4][16] Loss: 0.0637\n",
      "[4][17] Loss: 0.0669\n",
      "[4][18] Loss: 0.0819\n",
      "[4][19] Loss: 0.0707\n",
      "[4][20] Loss: 0.0506\n",
      "[4][21] Loss: 0.1055\n",
      "[4][22] Loss: 0.0997\n",
      "[4][23] Loss: 0.1008\n",
      "[4][24] Loss: 0.0711\n",
      "[4][25] Loss: 0.0605\n",
      "[4][26] Loss: 0.0780\n",
      "[4][27] Loss: 0.1146\n",
      "[4][28] Loss: 0.0645\n",
      "[4][29] Loss: 0.0806\n",
      "[4][30] Loss: 0.0507\n",
      "[4][31] Loss: 0.0606\n",
      "[4][32] Loss: 0.0732\n",
      "[4][33] Loss: 0.0683\n",
      "[4][34] Loss: 0.0790\n",
      "[4][35] Loss: 0.0720\n",
      "[5][0] Loss: 0.0555\n",
      "[5][1] Loss: 0.0506\n",
      "[5][2] Loss: 0.0472\n",
      "[5][3] Loss: 0.0547\n",
      "[5][4] Loss: 0.0787\n",
      "[5][5] Loss: 0.0731\n",
      "[5][6] Loss: 0.0609\n",
      "[5][7] Loss: 0.0444\n",
      "[5][8] Loss: 0.0463\n",
      "[5][9] Loss: 0.0540\n",
      "[5][10] Loss: 0.0662\n",
      "[5][11] Loss: 0.0528\n",
      "[5][12] Loss: 0.0603\n",
      "[5][13] Loss: 0.0851\n",
      "[5][14] Loss: 0.0620\n",
      "[5][15] Loss: 0.0523\n",
      "[5][16] Loss: 0.0634\n",
      "[5][17] Loss: 0.0528\n",
      "[5][18] Loss: 0.0512\n",
      "[5][19] Loss: 0.0517\n",
      "[5][20] Loss: 0.0447\n",
      "[5][21] Loss: 0.0529\n",
      "[5][22] Loss: 0.0661\n",
      "[5][23] Loss: 0.0749\n",
      "[5][24] Loss: 0.0529\n",
      "[5][25] Loss: 0.0458\n",
      "[5][26] Loss: 0.0759\n",
      "[5][27] Loss: 0.0585\n",
      "[5][28] Loss: 0.0380\n",
      "[5][29] Loss: 0.0543\n",
      "[5][30] Loss: 0.0548\n",
      "[5][31] Loss: 0.0625\n",
      "[5][32] Loss: 0.0518\n",
      "[5][33] Loss: 0.0533\n",
      "[5][34] Loss: 0.0384\n",
      "[5][35] Loss: 0.0465\n",
      "[6][0] Loss: 0.0506\n",
      "[6][1] Loss: 0.0444\n",
      "[6][2] Loss: 0.0402\n",
      "[6][3] Loss: 0.0454\n",
      "[6][4] Loss: 0.0425\n",
      "[6][5] Loss: 0.0354\n",
      "[6][6] Loss: 0.0258\n",
      "[6][7] Loss: 0.0519\n",
      "[6][8] Loss: 0.0435\n",
      "[6][9] Loss: 0.0629\n",
      "[6][10] Loss: 0.0650\n",
      "[6][11] Loss: 0.0527\n",
      "[6][12] Loss: 0.0219\n",
      "[6][13] Loss: 0.0481\n",
      "[6][14] Loss: 0.0336\n",
      "[6][15] Loss: 0.0299\n",
      "[6][16] Loss: 0.0455\n",
      "[6][17] Loss: 0.0202\n",
      "[6][18] Loss: 0.0221\n",
      "[6][19] Loss: 0.0587\n",
      "[6][20] Loss: 0.0443\n",
      "[6][21] Loss: 0.0122\n",
      "[6][22] Loss: 0.0537\n",
      "[6][23] Loss: 0.0388\n",
      "[6][24] Loss: 0.0605\n",
      "[6][25] Loss: 0.0536\n",
      "[6][26] Loss: 0.0412\n",
      "[6][27] Loss: 0.0401\n",
      "[6][28] Loss: 0.0520\n",
      "[6][29] Loss: 0.0432\n",
      "[6][30] Loss: 0.0335\n",
      "[6][31] Loss: 0.0450\n",
      "[6][32] Loss: 0.0564\n",
      "[6][33] Loss: 0.0417\n",
      "[6][34] Loss: 0.0338\n",
      "[6][35] Loss: 0.0674\n",
      "[7][0] Loss: 0.0316\n",
      "[7][1] Loss: 0.0280\n",
      "[7][2] Loss: 0.0401\n",
      "[7][3] Loss: 0.0320\n",
      "[7][4] Loss: 0.0374\n",
      "[7][5] Loss: 0.0302\n",
      "[7][6] Loss: 0.0148\n",
      "[7][7] Loss: 0.0308\n",
      "[7][8] Loss: 0.0202\n",
      "[7][9] Loss: 0.0304\n",
      "[7][10] Loss: 0.0402\n",
      "[7][11] Loss: 0.0324\n",
      "[7][12] Loss: 0.0331\n",
      "[7][13] Loss: 0.0316\n",
      "[7][14] Loss: 0.0292\n",
      "[7][15] Loss: 0.0464\n",
      "[7][16] Loss: 0.0400\n",
      "[7][17] Loss: 0.0339\n",
      "[7][18] Loss: 0.0455\n",
      "[7][19] Loss: 0.0332\n",
      "[7][20] Loss: 0.0319\n",
      "[7][21] Loss: 0.0409\n",
      "[7][22] Loss: 0.0359\n",
      "[7][23] Loss: 0.0219\n",
      "[7][24] Loss: 0.0356\n",
      "[7][25] Loss: 0.0393\n",
      "[7][26] Loss: 0.0371\n",
      "[7][27] Loss: 0.0353\n",
      "[7][28] Loss: 0.0307\n",
      "[7][29] Loss: 0.0193\n",
      "[7][30] Loss: 0.0328\n",
      "[7][31] Loss: 0.0419\n",
      "[7][32] Loss: 0.0274\n",
      "[7][33] Loss: 0.0129\n",
      "[7][34] Loss: 0.0459\n",
      "[7][35] Loss: 0.0380\n",
      "[8][0] Loss: 0.0144\n",
      "[8][1] Loss: 0.0207\n",
      "[8][2] Loss: 0.0277\n",
      "[8][3] Loss: 0.0277\n",
      "[8][4] Loss: 0.0219\n",
      "[8][5] Loss: 0.0277\n",
      "[8][6] Loss: 0.0326\n",
      "[8][7] Loss: 0.0284\n",
      "[8][8] Loss: 0.0257\n",
      "[8][9] Loss: 0.0304\n",
      "[8][10] Loss: 0.0176\n",
      "[8][11] Loss: 0.0224\n",
      "[8][12] Loss: 0.0125\n",
      "[8][13] Loss: 0.0175\n",
      "[8][14] Loss: 0.0186\n",
      "[8][15] Loss: 0.0210\n",
      "[8][16] Loss: 0.0211\n",
      "[8][17] Loss: 0.0288\n",
      "[8][18] Loss: 0.0320\n",
      "[8][19] Loss: 0.0340\n",
      "[8][20] Loss: 0.0330\n",
      "[8][21] Loss: 0.0277\n",
      "[8][22] Loss: 0.0254\n",
      "[8][23] Loss: 0.0150\n",
      "[8][24] Loss: 0.0227\n",
      "[8][25] Loss: 0.0273\n",
      "[8][26] Loss: 0.0216\n",
      "[8][27] Loss: 0.0413\n",
      "[8][28] Loss: 0.0302\n",
      "[8][29] Loss: 0.0215\n",
      "[8][30] Loss: 0.0300\n",
      "[8][31] Loss: 0.0189\n",
      "[8][32] Loss: 0.0261\n",
      "[8][33] Loss: 0.0354\n",
      "[8][34] Loss: 0.0190\n",
      "[8][35] Loss: 0.0156\n",
      "[9][0] Loss: 0.0210\n",
      "[9][1] Loss: 0.0137\n",
      "[9][2] Loss: 0.0200\n",
      "[9][3] Loss: 0.0155\n",
      "[9][4] Loss: 0.0191\n",
      "[9][5] Loss: 0.0227\n",
      "[9][6] Loss: 0.0275\n",
      "[9][7] Loss: 0.0140\n",
      "[9][8] Loss: 0.0168\n",
      "[9][9] Loss: 0.0106\n",
      "[9][10] Loss: 0.0145\n",
      "[9][11] Loss: 0.0207\n",
      "[9][12] Loss: 0.0134\n",
      "[9][13] Loss: 0.0195\n",
      "[9][14] Loss: 0.0173\n",
      "[9][15] Loss: 0.0138\n",
      "[9][16] Loss: 0.0156\n",
      "[9][17] Loss: 0.0237\n",
      "[9][18] Loss: 0.0144\n",
      "[9][19] Loss: 0.0159\n",
      "[9][20] Loss: 0.0202\n",
      "[9][21] Loss: 0.0261\n",
      "[9][22] Loss: 0.0115\n",
      "[9][23] Loss: 0.0293\n",
      "[9][24] Loss: 0.0228\n",
      "[9][25] Loss: 0.0250\n",
      "[9][26] Loss: 0.0186\n",
      "[9][27] Loss: 0.0150\n",
      "[9][28] Loss: 0.0200\n",
      "[9][29] Loss: 0.0141\n",
      "[9][30] Loss: 0.0235\n",
      "[9][31] Loss: 0.0310\n",
      "[9][32] Loss: 0.0185\n",
      "[9][33] Loss: 0.0188\n",
      "[9][34] Loss: 0.0157\n",
      "[9][35] Loss: 0.0155\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "logreg = LogisticClassifier(1108)\n",
    "logreg.to(device)\n",
    "opt = torch.optim.AdamW(logreg.parameters())\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    for j, (xb, yb) in enumerate(DataLoader(dataset, shuffle=True, batch_size=1024)):\n",
    "        opt.zero_grad()\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = logreg(xb)\n",
    "        loss = loss_fn(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(f'[{i}][{j}] Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv('stacked/test/densenet121_long_training.csv')\n",
    "f2 = pd.read_csv('stacked/test/densenet121_with_tricks.csv')\n",
    "f3 = pd.read_csv('stacked/test/densenet169.csv')\n",
    "X_test = np.column_stack([f.drop(columns='id_code').values for f in (f1, f2, f3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X_test).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903bcecd4eec4aa68bd43400e0871da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=156), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logreg.eval()\n",
    "    probs = []\n",
    "    for xb in tqdm(DataLoader(X_test, batch_size=128, shuffle=False)):\n",
    "        out = logreg(xb)\n",
    "        out = out.softmax(dim=-1).cpu().numpy()\n",
    "        probs.extend(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19897, 1108)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.row_stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'id_code': f1.id_code, 'sirna': stacked.argmax(axis=-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_csv = pd.read_csv(ROOT/'train.csv')\n",
    "tst_csv = pd.read_csv(ROOT/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_groups = np.zeros((1108,4), int)\n",
    "for sirna in range(1108):\n",
    "    grp = trn_csv.loc[trn_csv.sirna==sirna,:].plate.value_counts().index.values\n",
    "    assert len(grp) == 3\n",
    "    plate_groups[sirna,0:3] = grp\n",
    "    plate_groups[sirna,3] = 10 - grp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_exp = tst_csv.experiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_plate_probs = np.zeros((len(all_test_exp),4))\n",
    "for idx in range(len(all_test_exp)):\n",
    "    preds = sub.loc[tst_csv.experiment == all_test_exp[idx],'sirna'].values\n",
    "    pp_mult = np.zeros((len(preds),1108))\n",
    "    pp_mult[range(len(preds)),preds] = 1\n",
    "    \n",
    "    sub_test = tst_csv.loc[tst_csv.experiment == all_test_exp[idx],:]\n",
    "    assert len(pp_mult) == len(sub_test)\n",
    "    \n",
    "    for j in range(4):\n",
    "        mask = np.repeat(plate_groups[np.newaxis, :, j], len(pp_mult), axis=0) == \\\n",
    "               np.repeat(sub_test.plate.values[:, np.newaxis], 1108, axis=1)\n",
    "        \n",
    "        group_plate_probs[idx,j] = np.array(pp_mult)[mask].sum()/len(pp_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HEPG2-08</th>\n",
       "      <td>0.084914</td>\n",
       "      <td>0.093044</td>\n",
       "      <td>0.112918</td>\n",
       "      <td>0.709124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEPG2-09</th>\n",
       "      <td>0.160650</td>\n",
       "      <td>0.488267</td>\n",
       "      <td>0.181408</td>\n",
       "      <td>0.169675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEPG2-10</th>\n",
       "      <td>0.769856</td>\n",
       "      <td>0.072202</td>\n",
       "      <td>0.074007</td>\n",
       "      <td>0.083935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEPG2-11</th>\n",
       "      <td>0.828210</td>\n",
       "      <td>0.047920</td>\n",
       "      <td>0.055154</td>\n",
       "      <td>0.068716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-17</th>\n",
       "      <td>0.830325</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.057762</td>\n",
       "      <td>0.061372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-18</th>\n",
       "      <td>0.651310</td>\n",
       "      <td>0.130985</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.112014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-19</th>\n",
       "      <td>0.069495</td>\n",
       "      <td>0.079422</td>\n",
       "      <td>0.764440</td>\n",
       "      <td>0.086643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-20</th>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.031588</td>\n",
       "      <td>0.912455</td>\n",
       "      <td>0.030686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-21</th>\n",
       "      <td>0.080325</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.752708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-22</th>\n",
       "      <td>0.843863</td>\n",
       "      <td>0.041516</td>\n",
       "      <td>0.065884</td>\n",
       "      <td>0.048736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-23</th>\n",
       "      <td>0.855323</td>\n",
       "      <td>0.048226</td>\n",
       "      <td>0.054595</td>\n",
       "      <td>0.041856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-24</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.064545</td>\n",
       "      <td>0.049091</td>\n",
       "      <td>0.826364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-08</th>\n",
       "      <td>0.111011</td>\n",
       "      <td>0.629061</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>0.131769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-09</th>\n",
       "      <td>0.730804</td>\n",
       "      <td>0.093044</td>\n",
       "      <td>0.086721</td>\n",
       "      <td>0.089431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-10</th>\n",
       "      <td>0.731047</td>\n",
       "      <td>0.081227</td>\n",
       "      <td>0.089350</td>\n",
       "      <td>0.098375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-11</th>\n",
       "      <td>0.755941</td>\n",
       "      <td>0.068556</td>\n",
       "      <td>0.094150</td>\n",
       "      <td>0.081353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2OS-04</th>\n",
       "      <td>0.254513</td>\n",
       "      <td>0.221119</td>\n",
       "      <td>0.306859</td>\n",
       "      <td>0.217509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2OS-05</th>\n",
       "      <td>0.193254</td>\n",
       "      <td>0.180492</td>\n",
       "      <td>0.183227</td>\n",
       "      <td>0.443026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3\n",
       "HEPG2-08  0.084914  0.093044  0.112918  0.709124\n",
       "HEPG2-09  0.160650  0.488267  0.181408  0.169675\n",
       "HEPG2-10  0.769856  0.072202  0.074007  0.083935\n",
       "HEPG2-11  0.828210  0.047920  0.055154  0.068716\n",
       "HUVEC-17  0.830325  0.050542  0.057762  0.061372\n",
       "HUVEC-18  0.651310  0.130985  0.105691  0.112014\n",
       "HUVEC-19  0.069495  0.079422  0.764440  0.086643\n",
       "HUVEC-20  0.025271  0.031588  0.912455  0.030686\n",
       "HUVEC-21  0.080325  0.080325  0.086643  0.752708\n",
       "HUVEC-22  0.843863  0.041516  0.065884  0.048736\n",
       "HUVEC-23  0.855323  0.048226  0.054595  0.041856\n",
       "HUVEC-24  0.060000  0.064545  0.049091  0.826364\n",
       "RPE-08    0.111011  0.629061  0.128159  0.131769\n",
       "RPE-09    0.730804  0.093044  0.086721  0.089431\n",
       "RPE-10    0.731047  0.081227  0.089350  0.098375\n",
       "RPE-11    0.755941  0.068556  0.094150  0.081353\n",
       "U2OS-04   0.254513  0.221119  0.306859  0.217509\n",
       "U2OS-05   0.193254  0.180492  0.183227  0.443026"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(group_plate_probs, index=all_test_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 0 0 0 0 2 2 3 0 0 3 1 0 0 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "exp_to_group = group_plate_probs.argmax(1)\n",
    "print(exp_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_plate_group(pp_mult, idx):\n",
    "    sub_test = tst_csv.loc[tst_csv.experiment == all_test_exp[idx],:]\n",
    "    assert len(pp_mult) == len(sub_test)\n",
    "    mask = np.repeat(plate_groups[np.newaxis, :, exp_to_group[idx]], len(pp_mult), axis=0) != \\\n",
    "           np.repeat(sub_test.plate.values[:, np.newaxis], 1108, axis=1)\n",
    "    pp_mult[mask] = 0\n",
    "    return pp_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.set_index('id_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(all_test_exp)):\n",
    "    indexes = tst_csv.experiment == all_test_exp[idx]\n",
    "    preds = stacked[indexes, :].copy()\n",
    "    preds = select_plate_group(preds, idx)\n",
    "    sub.loc[tst_csv.id_code[indexes], 'sirna'] = preds.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='stack_leak.csv' target='_blank'>stack_leak.csv</a><br>"
      ],
      "text/plain": [
       "/home/ck/code/tasks/protein/stack_leak.csv"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "sub.to_csv('stack_leak.csv', index=False, columns=['id_code', 'sirna'])\n",
    "FileLink('stack_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

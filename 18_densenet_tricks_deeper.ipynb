{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: 18_densenet_tricks_deeper.ipynb -> densenet_tricks_deeper.py\r\n",
      "1 notebook(s) exported into folder: .\r\n"
     ]
    }
   ],
   "source": [
    "!python -m jupytools export -nb \"18_densenet_tricks_deeper.ipynb\" -o .\n",
    "!mv densenet_tricks_deeper.py densenet169.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/tanlikesmath/rcic-fastai-starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "from os.path import dirname, join\n",
    "from functools import reduce\n",
    "from pdb import set_trace\n",
    "\n",
    "import cv2 as cv\n",
    "import jupytools\n",
    "import jupytools.syspath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catalyst.utils import get_one_hot\n",
    "from imageio import imread\n",
    "import pretrainedmodels\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from visdom import Visdom\n",
    "\n",
    "jupytools.syspath.add(join(dirname(os.getcwd()), 'protein_project'))\n",
    "jupytools.syspath.add('rxrx1-utils')\n",
    "if jupytools.is_notebook():\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "else:\n",
    "    from tqdm import tqdm as tdqm\n",
    "    \n",
    "from basedir import ROOT, NUM_CLASSES\n",
    "from dataset import build_stats_index\n",
    "from lookahead import Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from augmentation import JoinChannels, SwapChannels, Resize, ToFloat, Rescale\n",
    "from augmentation import VerticalFlip, HorizontalFlip, PixelStatsNorm, composer\n",
    "from augmentation import UnconditionalTransform\n",
    "from augmentation import AugmentedImages, bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "default_open_fn = imread  # PIL.Image.open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class RxRxDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, meta_df, img_dir, sites=(1, 2), channels=(1, 2, 3, 4, 5, 6),\n",
    "                open_image=default_open_fn, n_classes=NUM_CLASSES, train=True,\n",
    "                flip_v=0.1, flip_h=0.1, resize=512, norm=True, rescale=False,\n",
    "                crop=False, hat=False, sigma_clip=False, label_smoothing=0.0):\n",
    "        \n",
    "        # data\n",
    "        self.records = meta_df.to_records(index=False)\n",
    "        self.img_dir = img_dir\n",
    "        self.sites = sites\n",
    "        self.channels = channels\n",
    "        self.n = len(self.records)\n",
    "        self.open_image = open_image\n",
    "        self.n_classes = n_classes\n",
    "        self.train = train\n",
    "        \n",
    "        # options\n",
    "        self.flip_v = flip_v\n",
    "        self.flip_h = flip_h\n",
    "        self.resize = resize\n",
    "        self.norm = norm\n",
    "        self.rescale = rescale\n",
    "        self.crop = crop\n",
    "        self.hat = hat\n",
    "        self.sigma_clip = sigma_clip\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        samples = {f'site{site}': self._get_site_image(index, site) for site in self.sites}\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def _get_image_path(self, index, channel, site):\n",
    "        r = self.records[index]\n",
    "        exp, plate, well = r.experiment, r.plate, r.well\n",
    "        subdir = 'train' if self.train else 'test'\n",
    "        path = f'{self.img_dir}/{subdir}/{exp}/Plate{plate}/{well}_s{site}_w{channel}.png'\n",
    "        return path\n",
    "        \n",
    "    def _get_site_image(self, index, site):\n",
    "        paths = [self._get_image_path(index, ch, site) for ch in self.channels]\n",
    "        images = [self.open_image(p) for p in paths]\n",
    "        image = self._concat(images)\n",
    "        image = self._augment(image)\n",
    "        sample = self._wrap_with_meta(image, self.records[index])\n",
    "        sample['site'] = site\n",
    "        return sample\n",
    "        \n",
    "    def _wrap_with_meta(self, image, meta):\n",
    "        if self.train:\n",
    "            sirna = meta.sirna\n",
    "            target = int(sirna)\n",
    "            onehot = get_one_hot(target, num_classes=self.n_classes,\n",
    "                                 smoothing=self.label_smoothing)\n",
    "            return {'features': image, 'targets': target, 'targets_one_hot': onehot,\n",
    "                    'id_code': meta.id_code}\n",
    "        else:\n",
    "            return {'features': image, 'id_code': meta.id_code}\n",
    "        \n",
    "    def _concat(self, images):\n",
    "        try:\n",
    "            img = np.stack(images)\n",
    "        except (TypeError, ValueError) as e:\n",
    "            print(f'Warning: cannot concatenate images! {e.__class__.__name__}: {e}')\n",
    "            for filename, image in zip(paths, images):\n",
    "                print(f'\\tpath={filename}, size={image.size}')\n",
    "            index = (index + 1) % len(self)\n",
    "            print(f'Skipping instance {index} and trying another one...')\n",
    "            return self[index]\n",
    "        finally:\n",
    "            for image in images:\n",
    "                if hasattr(image, 'close'):\n",
    "                    image.close()\n",
    "        return img\n",
    "                    \n",
    "    def _augment(self, image):\n",
    "        # OpenCV channels ordering\n",
    "        image = image.transpose(1, 2, 0)  # W x H x C\n",
    "        if self.resize:\n",
    "            image = cv.resize(image, (self.resize, self.resize))\n",
    "        if self.crop:\n",
    "            assert isinstance(self.crop, int), 'If crop provided, it should be integer'\n",
    "            assert self.crop < self.resize, 'Crop should be smaller than image size'\n",
    "            shift = np.random.randint(0, self.resize - self.crop - 1)\n",
    "            image = image[shift:(shift+self.crop), shift:(shift+self.crop), :]\n",
    "        if self.flip_v:\n",
    "            if bernoulli(self.flip_v) == 1:\n",
    "                image = cv.flip(image, 0)\n",
    "        if self.flip_h:\n",
    "            if bernoulli(self.flip_h) == 1:\n",
    "                image = cv.flip(image, 1)\n",
    "        if self.hat:\n",
    "            kernel = np.ones(self.hat, np.uint8)\n",
    "            image = cv.morphologyEx(image, cv.MORPH_TOPHAT, self.hat)\n",
    "        image = image.astype(np.float32)\n",
    "        if self.rescale:\n",
    "            image /= 255\n",
    "        if self.norm:\n",
    "            mean, std = image.mean(axis=(0, 1)), image.std(axis=(0, 1))\n",
    "            image = (image - mean)/(std + 1e-8)\n",
    "        if self.sigma_clip:\n",
    "            image = np.clip(image, -self.sigma_clip, self.sigma_clip)\n",
    "        image = image.transpose(2, 0, 1)  # C x W x H\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pipeline Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from split import StratifiedSplit\n",
    "splitter = StratifiedSplit()\n",
    "trn_df, val_df = splitter(pd.read_csv(ROOT/'train.csv')) \n",
    "tst_df = pd.read_csv(ROOT/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#trn_df = trn_df.head(100)\n",
    "#val_df = val_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "sz = 512\n",
    "\n",
    "trn_ds = RxRxDataset(\n",
    "    trn_df, ROOT,\n",
    "    crop=450, resize=sz, hat=(128, 128), norm=True, sigma_clip=5.0, label_smoothing=0.1)\n",
    "\n",
    "val_ds = RxRxDataset(\n",
    "    val_df, ROOT,\n",
    "    crop=450, resize=sz, hat=(128, 128), norm=True, sigma_clip=5.0, label_smoothing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def new_loader(ds, bs, drop_last=False, shuffle=True, num_workers=12):\n",
    "    return DataLoader(ds, batch_size=bs, drop_last=drop_last, \n",
    "                      shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def densenet(name='densenet121', n_classes=NUM_CLASSES):\n",
    "    model_fn = pretrainedmodels.__dict__[name]\n",
    "    model = model_fn(num_classes=1000, pretrained='imagenet')\n",
    "    new_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)\n",
    "    conv0 = model.features.conv0.weight\n",
    "    with torch.no_grad():\n",
    "        new_conv.weight[:, :] = torch.stack([torch.mean(conv0, 1)]*6, dim=1)\n",
    "    model.features.conv0 = new_conv\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# #export\n",
    "# from catalyst.contrib.modules import GlobalConcatPool2d\n",
    "# class DenseNet_TwoSites(nn.Module):\n",
    "#     def __init__(self, name, n_classes=NUM_CLASSES):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         base = densenet(name=name, n_classes=n_classes)\n",
    "#         feat_dim = base.last_linear.in_features\n",
    "        \n",
    "#         self.base = base \n",
    "#         self.pool = GlobalConcatPool2d()\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Linear(feat_dim * 2, feat_dim * 2),\n",
    "#             nn.BatchNorm1d(feat_dim * 2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(feat_dim * 2, n_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, s1, s2):\n",
    "#         f1 = self.base.features(s1)\n",
    "#         f2 = self.base.features(s2)\n",
    "#         f_merged = self.pool(f1 + f2)\n",
    "#         out = self.head(f_merged.squeeze())\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from catalyst.contrib.modules import GlobalConcatPool2d\n",
    "class DenseNet_TwoSites(nn.Module):\n",
    "    def __init__(self, name, n_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        \n",
    "        base = densenet(name=name, n_classes=n_classes)\n",
    "        feat_dim = base.last_linear.in_features\n",
    "        \n",
    "        self.base = base \n",
    "        self.pool = GlobalConcatPool2d()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feat_dim * 2, feat_dim),\n",
    "            nn.BatchNorm1d(feat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(feat_dim, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, s1, s2):\n",
    "        f1 = self.base.features(s1)\n",
    "        f2 = self.base.features(s2)\n",
    "        f_merged = self.pool(f1 + f2)\n",
    "        out = self.head(f_merged.squeeze())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def freeze_all(model):\n",
    "    for name, child in model.named_children():\n",
    "        print('Freezing layer:', name)\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_all(model):\n",
    "    for name, child in model.named_children():\n",
    "        print('Un-freezing layer:', name)\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def unfreeze_layers(model, names):\n",
    "    for name, child in model.named_children():\n",
    "        if name not in names:\n",
    "            continue\n",
    "        print('Un-freezing layer:', name)\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class CosineDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, total_steps,\n",
    "                 linear_start=0,\n",
    "                 linear_frac=0.1, min_lr=1e-6,\n",
    "                 last_epoch=-1):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.total_steps = total_steps\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_frac = linear_frac\n",
    "        self.min_lr = min_lr\n",
    "        self.linear_steps = total_steps * linear_frac\n",
    "        self.cosine_steps = total_steps - self.linear_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch\n",
    "        if step <= self.linear_steps:\n",
    "            b = self.linear_start\n",
    "            return [(step/self.linear_steps) * (base_lr - b) + b for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            t = self.last_epoch - self.linear_steps\n",
    "            T = self.cosine_steps\n",
    "            return [self.min_lr + (base_lr - self.min_lr)*(1 + np.cos(t*np.pi/T))/2\n",
    "                    for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer: base\n",
      "Freezing layer: pool\n",
      "Freezing layer: head\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "model = DenseNet_TwoSites('densenet169')\n",
    "state = torch.load('/home/ck/densenet169.pth', map_location=lambda l, s: l)\n",
    "model.load_state_dict(state)\n",
    "freeze_all(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class RollingLoss:\n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "        self.prev = 0\n",
    "    def __call__(self, curr, batch_no):\n",
    "        a = self.smooth\n",
    "        avg_loss = a*self.prev + (1 - a)*curr\n",
    "        debias_loss = avg_loss/(1 - a**batch_no)\n",
    "        self.prev = avg_loss\n",
    "        return debias_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_loaders(batch_size, drop_last=False):\n",
    "    trn_dl = new_loader(trn_ds, bs=batch_size, drop_last=drop_last, shuffle=True)\n",
    "    val_dl = new_loader(val_ds, bs=batch_size, drop_last=drop_last, shuffle=False)\n",
    "    return OrderedDict([('train', trn_dl), ('valid', val_dl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Checkpoint:\n",
    "    def __init__(self, output_dir):\n",
    "        if os.path.exists(output_dir):\n",
    "            print('Warning! Output folder already exists.')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.output_dir = output_dir\n",
    "    \n",
    "    def __call__(self, epoch, **objects):\n",
    "        filename = os.path.join(self.output_dir, f'train.{epoch}.pth')\n",
    "        checkpoint = {}\n",
    "        for k, v in objects.items():\n",
    "            if hasattr(v, 'state_dict'):\n",
    "                v = v.state_dict()\n",
    "            checkpoint[k] = v\n",
    "        torch.save(checkpoint, filename)\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, preds, one_hot_target):\n",
    "        preds = preds.log_softmax(dim=self.dim)\n",
    "        return torch.mean(torch.sum(-one_hot_target * preds, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = LabelSmoothingLoss()\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from ranger import Ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0926 11:50:39.439867 140376846067520 __init__.py:505] Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Output folder already exists.\n",
      "Un-freezing layer: base\n",
      "Un-freezing layer: pool\n",
      "Un-freezing layer: head\n",
      "Epoch [1/20]\n",
      "train metrics: accuracy=15.625%, loss=4.2803\n",
      "valid metrics: accuracy=14.063%, loss=3.5737\n",
      "Score improved!\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [2/20]\n",
      "train metrics: accuracy=14.583%, loss=4.2737\n",
      "valid metrics: accuracy=13.542%, loss=3.5548\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [3/20]\n",
      "train metrics: accuracy=18.229%, loss=4.1645\n",
      "valid metrics: accuracy=12.500%, loss=3.4200\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [4/20]\n",
      "train metrics: accuracy=22.396%, loss=4.0165\n",
      "valid metrics: accuracy=14.583%, loss=3.3486\n",
      "Score improved!\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [5/20]\n",
      "train metrics: accuracy=23.438%, loss=3.9154\n",
      "valid metrics: accuracy=14.583%, loss=3.5636\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [6/20]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cad57d6bcd16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;31m#if (epoch == (warmup_steps - 1)) and batch_no == (len(loader) - 1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai_10/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tasks/protein/ranger.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m#compute mean moving avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#export\n",
    "epochs = 20\n",
    "patience = 15\n",
    "\n",
    "#opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "#opt = Lookahead(opt)\n",
    "opt = Ranger(model.parameters(), lr=3e-4)\n",
    "model = model.to(device)\n",
    "rolling_loss = dict(train=RollingLoss(), valid=RollingLoss())\n",
    "steps = dict(train=0, valid=0)\n",
    "\n",
    "trials = 0\n",
    "best_metric = -np.inf\n",
    "history = []\n",
    "stop = False\n",
    "\n",
    "vis = Visdom(server='0.0.0.0', port=9090,\n",
    "             username=os.environ['VISDOM_USERNAME'],\n",
    "             password=os.environ['VISDOM_PASSWORD'])\n",
    "\n",
    "# loaders = create_loaders(batch_size=7)\n",
    "\n",
    "# sched = CosineDecay(opt, total_steps=len(loaders['train']) * epochs, \n",
    "#                     linear_start=1e-6, linear_frac=0.1, min_lr=3e-6)\n",
    "\n",
    "loaders = create_loaders(batch_size=6, drop_last=True)\n",
    "\n",
    "eta_min = 3e-6\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "# sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#     opt, T_0=len(loaders['train']), T_mult=2, eta_min=eta_min\n",
    "# )\n",
    "\n",
    "sched = CosineDecay(\n",
    "    opt, total_steps=len(loaders['train']) * epochs,\n",
    "    linear_start=eta_min, linear_frac=0.2, min_lr=3e-6)\n",
    "\n",
    "warmup_steps = 4\n",
    "\n",
    "checkpoint = Checkpoint('densenet169')\n",
    "\n",
    "unfreeze_all(model)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'Epoch [{epoch}/{epochs}]')\n",
    "    \n",
    "#     if epoch == 1:\n",
    "#         unfreeze_layers(model, ['head'])\n",
    "#     elif epoch == warmup_steps:\n",
    "#         unfreeze_all(model)\n",
    "#         sched = CosineDecay(\n",
    "#             opt, total_steps=len(loaders['train']) * (epochs - warmup_steps + 1), \n",
    "#             linear_start=eta_min, linear_frac=0.1, min_lr=3e-6)\n",
    "#         global_step = 0\n",
    "    \n",
    "    iteration = dict(epoch=epoch, train_loss=list(), valid_loss=list())\n",
    "    \n",
    "    for name, loader in loaders.items():\n",
    "        is_training = name == 'train'\n",
    "        count = 0\n",
    "        metric = 0.0\n",
    "        \n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            for batch_no, batch in enumerate(loader):\n",
    "                steps[name] += 1\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # y = batch['site1']['targets'].to(device)\n",
    "                y = batch['site1']['targets_one_hot'].to(device)\n",
    "                \n",
    "                out = model(\n",
    "                    batch['site1']['features'].to(device),\n",
    "                    batch['site2']['features'].to(device)\n",
    "                )\n",
    "                \n",
    "                if is_training:\n",
    "                    global_step += 1\n",
    "                    loss = loss_fn(out, y)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    \n",
    "                    #if (epoch == (warmup_steps - 1)) and batch_no == (len(loader) - 1):\n",
    "                    #    pass  # skip\n",
    "                    #else:\n",
    "                    #    sched.step(global_step)\n",
    "                    \n",
    "                    sched.step()\n",
    "                    curr_lr = opt.param_groups[0]['lr']\n",
    "                    vis.line(X=[steps[name]], Y=[curr_lr], win='lr', name='lr', update='append')    \n",
    "                \n",
    "                avg_loss = rolling_loss[name](loss.item(), steps[name])\n",
    "                iteration[f'{name}_loss'].append(avg_loss)\n",
    "                y_pred = out.softmax(dim=1).argmax(dim=1)\n",
    "                y_true = batch['site1']['targets'].to(device)\n",
    "                acc = (y_pred == y_true).float().mean().item()\n",
    "                metric += acc\n",
    "                count += len(batch)\n",
    "                vis.line(X=[steps[name]], Y=[avg_loss], name=f'{name}_loss', \n",
    "                         win=f'{name}_loss', update='append', \n",
    "                         opts=dict(title=f'Running Loss [{name}]'))\n",
    "        \n",
    "        metric /= count\n",
    "        iteration[f'{name}_acc'] = metric\n",
    "        vis.line(X=[epoch], Y=[avg_loss], name=f'{name}', win='avg_loss',\n",
    "                 update='append', opts=dict(title='Average Epoch Loss'))\n",
    "        vis.line(X=[epoch], Y=[metric], name=f'{name}', win='accuracy', \n",
    "                 update='append', opts=dict(title=f'Accuracy'))\n",
    "        \n",
    "        last_loss = iteration[f'{name}_loss'][-1]\n",
    "        \n",
    "        print(f'{name} metrics: accuracy={metric:2.3%}, loss={last_loss:.4f}')\n",
    "          \n",
    "        if is_training:\n",
    "            pass\n",
    "          \n",
    "        else:\n",
    "            if metric > best_metric:\n",
    "                trials = 0\n",
    "                best_metric = metric\n",
    "                print('Score improved!')\n",
    "                checkpoint(epoch, model=model, opt=opt)\n",
    "\n",
    "            else:\n",
    "                trials += 1\n",
    "                if trials >= patience:\n",
    "                    stop = True\n",
    "                    break\n",
    "    \n",
    "    history.append(iteration)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    \n",
    "    if stop:\n",
    "        print(f'Early stopping on epoch: {epoch}')\n",
    "        break\n",
    "\n",
    "torch.save(history, f'{checkpoint.output_dir}/history.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer: base\n",
      "Freezing layer: pool\n",
      "Freezing layer: head\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = DenseNet_TwoSites('densenet169')\n",
    "model = model.to(device)\n",
    "state = torch.load('densenet169/train.11.pth', map_location=lambda l, s: l)\n",
    "model.load_state_dict(state['model'])\n",
    "freeze_all(model)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_ds = RxRxDataset(\n",
    "    tst_df, ROOT,\n",
    "    crop=450, resize=sz, hat=(128, 128), norm=True, sigma_clip=5.0, \n",
    "    label_smoothing=None, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b641473ef043a7a54e7eb51c094f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=311), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_dl = new_loader(tst_ds, shuffle=False, bs=64)\n",
    "    probs = {}\n",
    "    for batch in tqdm(test_dl):\n",
    "        s1 = batch['site1']['features']\n",
    "        s2 = batch['site2']['features']\n",
    "        out = model(s1.to(device), s2.to(device))\n",
    "        y_prob = out.softmax(dim=-1).cpu().numpy()\n",
    "        probs.update(dict(zip(batch['site1']['id_code'], y_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(probs)\n",
    " .T\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'id_code'})\n",
    " .to_csv('stacked/test/densenet169.csv', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8252bd9fe11467fa418c24a4fe804d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sup_df = pd.read_csv(ROOT/'train.csv')\n",
    "\n",
    "trn_ds = RxRxDataset(\n",
    "    sup_df, ROOT,\n",
    "    crop=450, resize=sz, hat=(128, 128), norm=True, sigma_clip=5.0, label_smoothing=0.1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_dl = new_loader(trn_ds, shuffle=False, bs=64)\n",
    "    probs = {}\n",
    "    for batch in tqdm(test_dl):\n",
    "        s1 = batch['site1']['features']\n",
    "        s2 = batch['site2']['features']\n",
    "        out = model(s1.to(device), s2.to(device))\n",
    "        y_prob = out.softmax(dim=-1).cpu().numpy()\n",
    "        probs.update(dict(zip(batch['site1']['id_code'], y_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(probs)\n",
    " .T\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'id_code'})\n",
    " .to_csv('stacked/train/densenet169.csv', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = pd.DataFrame(probs).T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_code = pd.DataFrame(probs).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked = np.row_stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'id_code': id_code, 'sirna': stacked.argmax(axis=-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_csv = pd.read_csv(ROOT/'train.csv')\n",
    "tst_csv = pd.read_csv(ROOT/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_groups = np.zeros((1108,4), int)\n",
    "for sirna in range(1108):\n",
    "    grp = trn_csv.loc[trn_csv.sirna==sirna,:].plate.value_counts().index.values\n",
    "    assert len(grp) == 3\n",
    "    plate_groups[sirna,0:3] = grp\n",
    "    plate_groups[sirna,3] = 10 - grp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_exp = tst_csv.experiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_plate_probs = np.zeros((len(all_test_exp),4))\n",
    "for idx in range(len(all_test_exp)):\n",
    "    preds = sub.loc[tst_csv.experiment == all_test_exp[idx],'sirna'].values\n",
    "    pp_mult = np.zeros((len(preds),1108))\n",
    "    pp_mult[range(len(preds)),preds] = 1\n",
    "    \n",
    "    sub_test = tst_csv.loc[tst_csv.experiment == all_test_exp[idx],:]\n",
    "    assert len(pp_mult) == len(sub_test)\n",
    "    \n",
    "    for j in range(4):\n",
    "        mask = np.repeat(plate_groups[np.newaxis, :, j], len(pp_mult), axis=0) == \\\n",
    "               np.repeat(sub_test.plate.values[:, np.newaxis], 1108, axis=1)\n",
    "        \n",
    "        group_plate_probs[idx,j] = np.array(pp_mult)[mask].sum()/len(pp_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HEPG2-08</th>\n",
       "      <td>0.149051</td>\n",
       "      <td>0.132791</td>\n",
       "      <td>0.154472</td>\n",
       "      <td>0.563686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEPG2-09</th>\n",
       "      <td>0.190433</td>\n",
       "      <td>0.389892</td>\n",
       "      <td>0.235560</td>\n",
       "      <td>0.184116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEPG2-10</th>\n",
       "      <td>0.714801</td>\n",
       "      <td>0.088448</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.110108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEPG2-11</th>\n",
       "      <td>0.736890</td>\n",
       "      <td>0.080470</td>\n",
       "      <td>0.094937</td>\n",
       "      <td>0.087703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-17</th>\n",
       "      <td>0.743682</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.092960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-18</th>\n",
       "      <td>0.607046</td>\n",
       "      <td>0.125565</td>\n",
       "      <td>0.139115</td>\n",
       "      <td>0.128275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-19</th>\n",
       "      <td>0.104693</td>\n",
       "      <td>0.116426</td>\n",
       "      <td>0.657942</td>\n",
       "      <td>0.120939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-20</th>\n",
       "      <td>0.046029</td>\n",
       "      <td>0.033394</td>\n",
       "      <td>0.881769</td>\n",
       "      <td>0.038809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-21</th>\n",
       "      <td>0.119134</td>\n",
       "      <td>0.093863</td>\n",
       "      <td>0.102888</td>\n",
       "      <td>0.684116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-22</th>\n",
       "      <td>0.767148</td>\n",
       "      <td>0.063177</td>\n",
       "      <td>0.096570</td>\n",
       "      <td>0.073105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-23</th>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.084622</td>\n",
       "      <td>0.090992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUVEC-24</th>\n",
       "      <td>0.077273</td>\n",
       "      <td>0.093636</td>\n",
       "      <td>0.081818</td>\n",
       "      <td>0.747273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-08</th>\n",
       "      <td>0.161552</td>\n",
       "      <td>0.490975</td>\n",
       "      <td>0.175993</td>\n",
       "      <td>0.171480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-09</th>\n",
       "      <td>0.610659</td>\n",
       "      <td>0.156278</td>\n",
       "      <td>0.114724</td>\n",
       "      <td>0.118338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-10</th>\n",
       "      <td>0.606498</td>\n",
       "      <td>0.139892</td>\n",
       "      <td>0.126354</td>\n",
       "      <td>0.127256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RPE-11</th>\n",
       "      <td>0.653565</td>\n",
       "      <td>0.104205</td>\n",
       "      <td>0.122486</td>\n",
       "      <td>0.119744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2OS-04</th>\n",
       "      <td>0.216606</td>\n",
       "      <td>0.249097</td>\n",
       "      <td>0.304152</td>\n",
       "      <td>0.230144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2OS-05</th>\n",
       "      <td>0.187785</td>\n",
       "      <td>0.221513</td>\n",
       "      <td>0.214221</td>\n",
       "      <td>0.376481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3\n",
       "HEPG2-08  0.149051  0.132791  0.154472  0.563686\n",
       "HEPG2-09  0.190433  0.389892  0.235560  0.184116\n",
       "HEPG2-10  0.714801  0.088448  0.086643  0.110108\n",
       "HEPG2-11  0.736890  0.080470  0.094937  0.087703\n",
       "HUVEC-17  0.743682  0.080325  0.083032  0.092960\n",
       "HUVEC-18  0.607046  0.125565  0.139115  0.128275\n",
       "HUVEC-19  0.104693  0.116426  0.657942  0.120939\n",
       "HUVEC-20  0.046029  0.033394  0.881769  0.038809\n",
       "HUVEC-21  0.119134  0.093863  0.102888  0.684116\n",
       "HUVEC-22  0.767148  0.063177  0.096570  0.073105\n",
       "HUVEC-23  0.726115  0.098271  0.084622  0.090992\n",
       "HUVEC-24  0.077273  0.093636  0.081818  0.747273\n",
       "RPE-08    0.161552  0.490975  0.175993  0.171480\n",
       "RPE-09    0.610659  0.156278  0.114724  0.118338\n",
       "RPE-10    0.606498  0.139892  0.126354  0.127256\n",
       "RPE-11    0.653565  0.104205  0.122486  0.119744\n",
       "U2OS-04   0.216606  0.249097  0.304152  0.230144\n",
       "U2OS-05   0.187785  0.221513  0.214221  0.376481"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(group_plate_probs, index=all_test_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 0 0 0 0 2 2 3 0 0 3 1 0 0 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "exp_to_group = group_plate_probs.argmax(1)\n",
    "print(exp_to_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_plate_group(pp_mult, idx):\n",
    "    sub_test = tst_csv.loc[tst_csv.experiment == all_test_exp[idx],:]\n",
    "    assert len(pp_mult) == len(sub_test)\n",
    "    mask = np.repeat(plate_groups[np.newaxis, :, exp_to_group[idx]], len(pp_mult), axis=0) != \\\n",
    "           np.repeat(sub_test.plate.values[:, np.newaxis], 1108, axis=1)\n",
    "    pp_mult[mask] = 0\n",
    "    return pp_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.set_index('id_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(all_test_exp)):\n",
    "    indexes = tst_csv.experiment == all_test_exp[idx]\n",
    "    preds = stacked[indexes, :].copy()\n",
    "    preds = select_plate_group(preds, idx)\n",
    "    sub.loc[tst_csv.id_code[indexes], 'sirna'] = preds.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='dense169_leak.csv' target='_blank'>dense169_leak.csv</a><br>"
      ],
      "text/plain": [
       "/home/ck/code/tasks/protein/dense169_leak.csv"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "sub.to_csv('dense169_leak.csv', index=False, columns=['id_code', 'sirna'])\n",
    "FileLink('dense169_leak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
